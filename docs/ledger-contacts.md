---
layout: text
title: Contacts Ledger
permalink: /docs/ledger-contacts/
---

# Contacts Ledger

```
┌─────────────────────────────────────────────────────────────────────────────┐
│  CONTACTS & COLLABORATORS                                   rev. 2026-01-02 │
│  potential advisors, researchers, labs                                      │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Status Key

```
[★]   primary target
[◐]   secondary option
[?]   unknown fit
[✗]   ruled out
[→]   contacted
[←]   received response
[✓]   positive interaction
```

---

## I. Primary Targets

### Daniel Soudry [★]

```
┌─────────────────────────────────────────────────────────────────────────────┐
│ PROFILE                                                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│ Name:        Daniel Soudry                                                  │
│ Institution: Technion, EE Department                                        │
│ Lab:         Neural Network Theory & Applications                           │
│ Website:     https://soudry.github.io                                       │
│ Email:       daniel.soudry@technion.ac.il                                   │
│ H-index:     ~45                                                            │
├─────────────────────────────────────────────────────────────────────────────┤
│ STATUS                                                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│ Contact:     [ ] not yet contacted                                          │
│ Fit:         [★] excellent - quantization theory + my linguistics           │
│ Openings:    [?] check website / email                                      │
│ Funding:     [?] likely has grants                                          │
├─────────────────────────────────────────────────────────────────────────────┤
│ KEY PAPERS                                                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│ • Quantized Neural Networks (JMLR 2017) - foundational                      │
│ • FP8 Training (ICLR 2025 spotlight) - current frontier                     │
│ • FP4 for Trillion-Token LLMs (NeurIPS 2025)                                │
├─────────────────────────────────────────────────────────────────────────────┤
│ PITCH ANGLE                                                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│ "Extend your quantization theory into multilingual domain using             │
│  linguistic typology as new theoretical variable. I bring the linguistics." │
├─────────────────────────────────────────────────────────────────────────────┤
│ CONCERNS                                                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│ • May not be interested in linguistics angle                                │
│ • May have too many students already                                        │
│ • Competitive lab - need strong preliminary results                         │
└─────────────────────────────────────────────────────────────────────────────┘
```

**Lab Members to Know:**
- Brian Chmiel - FP4/FP8 lead author
- Itay Hubara - Quantization pioneer (now NVIDIA?)
- Yoav Banner - Post-training quantization

**Action Items:**
- [ ] Read all Soudry papers
- [ ] Complete zero-cost analysis (A01-A05)
- [ ] Draft email with preliminary results
- [ ] Check PhD admission deadlines

---

### Yonatan Belinkov [◐]

```
┌─────────────────────────────────────────────────────────────────────────────┐
│ Name:        Yonatan Belinkov                                               │
│ Institution: Technion, CS Department                                        │
│ Lab:         NLP & Interpretability                                         │
│ Website:     https://belinkov.com                                           │
│ Fit:         [◐] probing methods, less quantization focus                   │
├─────────────────────────────────────────────────────────────────────────────┤
│ PITCH ANGLE                                                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│ "Use probing classifiers to understand what linguistic information is       │
│  lost during quantization, across languages."                               │
├─────────────────────────────────────────────────────────────────────────────┤
│ PROS                                                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│ • Probing methodology expert                                                │
│ • Multilingual NLP interest                                                 │
│ • Interpretability angle                                                    │
├─────────────────────────────────────────────────────────────────────────────┤
│ CONS                                                                        │
├─────────────────────────────────────────────────────────────────────────────┤
│ • Not quantization-focused                                                  │
│ • May see it as peripheral to his interests                                 │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

### Roi Reichart [◐]

```
┌─────────────────────────────────────────────────────────────────────────────┐
│ Name:        Roi Reichart                                                   │
│ Institution: Technion, IE&M Department                                      │
│ Lab:         NLP                                                            │
│ Fit:         [◐] multilingual NLP, less efficiency focus                    │
├─────────────────────────────────────────────────────────────────────────────┤
│ PITCH ANGLE                                                                 │
├─────────────────────────────────────────────────────────────────────────────┤
│ "Fairness in multilingual model deployment - quantization as equity issue." │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## II. Other Israeli Labs

### Hebrew University

| Researcher | Area | Fit | Notes |
|------------|------|-----|-------|
| Omri Abend | Semantics, Multilingual | [◐] | Semantic parsing |
| Daphna Weinshall | Vision, Learning | [?] | Less NLP |
| Amnon Shashua | ML Theory | [?] | Industry-focused |

### Tel Aviv University

| Researcher | Area | Fit | Notes |
|------------|------|-----|-------|
| Lior Wolf | Deep Learning | [?] | Vision-heavy |
| Jonathan Berant | NLP | [◐] | QA systems |
| Yoav Goldberg | NLP | [?] | Now at AI21 |

### Bar-Ilan University

| Researcher | Area | Fit | Notes |
|------------|------|-----|-------|
| Ido Dagan | NLP | [◐] | Textual entailment |
| Yoav Goldberg | NLP | [?] | Split with TAU |

### Ben-Gurion University

| Researcher | Area | Fit | Notes |
|------------|------|-----|-------|
| Michael Elhadad | NLP | [?] | Hebrew NLP |

---

## III. International (Backup)

### Efficiency & Quantization

| Researcher | Institution | Fit | Notes |
|------------|-------------|-----|-------|
| Dan Alistarh | IST Austria | [◐] | GPTQ author |
| Song Han | MIT | [?] | AWQ, very competitive |
| Tri Dao | Princeton | [?] | FlashAttention |
| Tim Dettmers | UW | [◐] | bitsandbytes |

### Multilingual NLP

| Researcher | Institution | Fit | Notes |
|------------|-------------|-----|-------|
| Graham Neubig | CMU | [◐] | Low-resource NLP |
| Yulia Tsvetkov | UW | [◐] | Multilingual |
| Rada Mihalcea | Michigan | [?] | Multilingual |

---

## IV. Industry Contacts

| Company | Relevance | Contact | Notes |
|---------|-----------|---------|-------|
| NVIDIA | Quantization | ? | Hubara went there |
| Intel | Quantization | ? | Israel lab |
| AI21 | Hebrew NLP | ? | Local |
| Cohere | Aya model | ? | Open to research |

---

## V. Contact Log

```
┌─────────────────────────────────────────────────────────────────────────────┐
│ DATE       │ CONTACT        │ ACTION    │ RESPONSE │ FOLLOW-UP              │
├────────────┼────────────────┼───────────┼──────────┼────────────────────────┤
│ [PENDING]  │ Daniel Soudry  │ email     │ —        │ after A05 complete     │
│            │                │           │          │                        │
└────────────┴────────────────┴───────────┴──────────┴────────────────────────┘
```

---

## VI. Email Templates

### Initial Contact (Soudry)

```
Subject: PhD inquiry — Linguistic typology and LLM quantization

Dear Prof. Soudry,

I'm [name], a physicist with computational linguistics background,
interested in joining your group for Fall 2026.

Your FP4/FP8 quantization work raises a question I've been exploring:
why do some languages degrade more under quantization? Re-analyzing
Marchisio et al.'s data, I find tokenization fertility (r=0.61) and
script type (r=0.58) predict degradation better than training data
size (r=-0.42).

I'd like to formalize this as a threshold function b*(L) predicting
minimum precision per language, then design linguistically-informed
quantization strategies. This extends your theoretical framework into
multilingual deployment—a high-impact application.

Attached is a 2-page proposal. Would you be open to a brief call?

Best,
[name]

Attachment: proposal-soudry.pdf
```

### Follow-up (If No Response)

```
Subject: Re: PhD inquiry — following up

Dear Prof. Soudry,

Following up on my email from [date]. I've since completed preliminary
analysis showing [specific result]. Happy to share the full results.

Best,
[name]
```

---

*Last updated: 2026-01-02*
