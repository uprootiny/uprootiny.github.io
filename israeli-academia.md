---
layout: text
title: Israeli Academia Guide
permalink: /israeli-academia/
---

# Israeli Academia: PhD Programs & Research

*Complete guide to graduate programs in Math, CS, and AI at Israeli universities.*

---

## TL;DR — What To Do Now

**It's January 2026. For Fall 2026 entry:**

| If you want to... | Do this NOW |
|-------------------|-------------|
| **Apply formally** | Email 2-3 advisors this month; deadlines are March-May |
| **Avoid paperwork** | Start a remote collaboration instead (see [low-bureaucracy paths](#if-you-refuse-to-provide-records)) |
| **Just explore** | Read advisor papers, attend online seminars, no commitment |

**Key insight:** The formal application is not the bottleneck. Finding an advisor who wants to work with you is. Everything else follows.

---

## Timelines: When to Engage

*Your first question answered: when do I need to act?*

### Critical Deadlines (Fall 2026 Entry)

| Date | Action | University |
|------|--------|------------|
| **NOW** | Start reading faculty papers, identify 2-4 potential advisors | All |
| **Oct 2025** | Begin emailing advisors (brief, specific) | All |
| **Nov 5, 2025** | Weizmann application opens | Weizmann |
| **Nov-Dec 2025** | Request transcripts, ask recommenders | All |
| **Feb 15, 2026** | TAU housing deadline | TAU |
| **March 28, 2026** | **Weizmann deadline** | Weizmann |
| **March 31, 2026** | **HUJI deadline** | HUJI |
| **April 2026** | Technion deadline | Technion |
| **May 31, 2026** | **TAU deadline** | TAU |
| **May-July 2026** | Interviews, decisions | All |

**Bottom line:** If you want to start Fall 2026, you need to be actively emailing advisors by October 2025 and submitting applications by March 2026.

### Time Investment

| Component | Time | Notes |
|-----------|------|-------|
| Program research | 2-3 weeks | Read faculty publications |
| Statement of purpose | 2-3 weeks | Expect 5+ drafts |
| Research proposal (PhD) | 2-4 weeks | Required for most PhD programs |
| CV preparation | 1 week | Academic format |
| Recommendation letters | 4 weeks lead time | Ask early |
| Application forms | 1 week | Tedious but important |

---

## Credentials: What You'll Need

*Your second question addressed: what records do they require?*

### Typical Requirements

| Document | Required By | Notes |
|----------|-------------|-------|
| **Official transcripts** | All | Usually need official/certified copies |
| **Degree certificate** | All | Or expected completion letter |
| **Statement of purpose** | All | 1-2 pages, program-specific |
| **CV** | All | Academic format with photo (Israel norm) |
| **Recommendation letters** | All | 2-3, academic preferred |
| **TOEFL/IELTS** | Most | If English not native; ~80 iBT / 6.5 IELTS |
| **GRE** | Some | Technion EE/Aero require; others optional |
| **Research proposal** | PhD | 2-5 pages on proposed research |
| **Application fee** | Most | $100-150 (Weizmann: free) |

### If You Refuse to Provide Records

*Reality check: formal PhD programs require documentation. But there are paths with lower bureaucratic burden.*

**Absolute minimums (can't avoid):**
- Some form of identity verification
- Proof you can do the work (publications, code, or demonstrated expertise)
- An advisor willing to vouch for you

**What you can often skip:**
- GRE — Most Israeli programs don't require it (except Technion EE/Aero)
- TOEFL — If you're clearly fluent (interview demonstrates this)
- Official transcripts — Some advisors will advocate for you without seeing grades
- Application fee — Weizmann is free; others may waive with explanation

**Alternative paths with less paperwork:**

| Path | Bureaucracy | How it works |
|------|-------------|--------------|
| **Research visitor** | Low | Email advisor directly, propose 3-6 month visit, no formal admission |
| **Collaboration** | Very low | Co-author papers remotely, build relationship, formalize later |
| **Industry → Academia** | Medium | Join Israeli tech company, publish, transition to research role |
| **Conference networking** | Very low | Present at Israeli venues (ISCOL, Reversim), meet faculty in person |
| **Postdoc** | Medium | If you have PhD elsewhere, postdoc applications are simpler |

**The "advisor-first" strategy:**

1. Find an advisor whose work you genuinely admire
2. Email them with a specific research idea (not "I want to join your lab")
3. If they're interested, they'll tell you what paperwork actually matters
4. Let them navigate the bureaucracy for you

*Many Israeli academics are pragmatic — if you can do the work, paperwork becomes a formality they'll help you handle.*

### If You're Missing Credentials

**No official transcripts available?**
- Contact universities directly — some accept unofficial copies with explanation
- Some Israeli universities are flexible with international applicants

**No academic recommenders?**
- Research supervisors, thesis advisors are preferred
- Industry supervisors acceptable for some programs
- Explain gaps in statement of purpose

**GPA below threshold?**
- Weizmann requires 90+ (Israeli scale) but accepts explanation letters
- Strong statement + advisor pre-approval can overcome GPA issues
- Consider MSc first to establish track record

---

## Quick Comparison: Universities

| University | Deadline | Tuition | Strengths | Recruiting |
|------------|----------|---------|-----------|------------|
| **Weizmann** | March 28 | **Free + stipend** | Math, TCS, theory | ~10 PhD/year |
| **HUJI** | March 31 | $10-30k | NLP (Schwartz ✅), neuroscience | Active |
| **TAU** | May 31 | $10-30k | Applied ML, industry ties | Active |
| **Technion** | April | Varies | Deep learning (Soudry ✅), optimization | Active |
| **BIU** | Varies | $5-12k | NLP (world-class), lowest tuition | High standards |
| **BGU** | Oct 2025 | $10-30k | Cyber security, water | Smaller |

---

## Actively Recruiting Advisors (January 2026)

*These explicitly state they want PhD students — prioritize these.*

| Advisor | University | Field | Source |
|---------|-----------|-------|--------|
| **Daniel Soudry** | Technion EE | Deep learning theory | [soudry.github.io](https://soudry.github.io/) |
| **Roy Schwartz** | HUJI CS | Efficient NLP, Green AI | [schwartz-lab-huji.github.io](https://schwartz-lab-huji.github.io/) |
| **Weizmann Math/CS** | Weizmann | Math, TCS | [weizmann.ac.il/math/join-us](https://www.weizmann.ac.il/math/join-us) |

### Conditional / Contact Carefully

| Advisor | Status |
|---------|--------|
| **Yonatan Belinkov (Technion)** | Not accepting 2025-26 (sabbatical); **open for 2026-27** |
| **Yoav Goldberg (BIU)** | [Note to prospective students](http://u.cs.biu.ac.il/~yogo/note-to-grads.html) — high standards |

---

## Advisor Deep Dives

*What their students' careers tell us about working with them.*

### Daniel Soudry (Technion) ✅ Actively Recruiting

**Research:** Neural network theory, quantization, optimization dynamics, generalization

**Lab metrics:**
| Metric | Value |
|--------|-------|
| Papers 2023-2025 | 25+ (NeurIPS, ICML, ICLR) |
| Spotlights | 6 (exceptional acceptance rate) |
| Current PhDs | 3 |
| Current MSc | 6 |
| Alumni PhDs | 10 |
| Funding | ERC A-B-C-Deep, Intel (5 grants), ISF |

**Recent output (2024-2025):**
- NeurIPS 2025: 6 papers (2 spotlights) — "Temperature is All You Need for Generalization in Langevin Dynamics", "FP4 All the Way: Fully Quantized Training of LLMs"
- ICLR 2025 spotlight: "Scaling FP8 training to trillion-token LLMs"
- ICML 2025: "When Diffusion Models Memorize..."
- ALT 2026: 2 papers on continual learning
- Member of Israel Young Academy

**Research pillars (from 25 papers):**

| Pillar | Focus | Key People |
|--------|-------|------------|
| **Quantization** | FP4, FP8, binarized networks | Brian Chmiel, Maxim Fishman, Itay Hubara |
| **Implicit bias** | Why GD finds "good" solutions | Mor Shpigel Nacson (alum) |
| **Continual learning** | Catastrophic forgetting theory | Itay Evron (alum, now Meta) |
| **Generalization** | Why overparameterized nets work | Itamar Harel, Gon Buzaglo |
| **Architecture** | Alias-free networks, shift invariance | Hagay Michaeli |

**Current students:** Matan Haroush, Hagay Michaeli, Mikey Shechter (PhD); Gil Denekamp, Itay Lamprecht, Ran Levinstein, Matan Tsipori, Ido Blayberg, Gilad Karpel (MSc)

**Alumni thesis summaries:**

| Name | Thesis/Focus | Duration | Where Now |
|------|--------------|----------|-----------|
| **Elad Hoffer** | "Deep Learning: Rethinking Common Practices" — batch size ≠ generalization, fixed classifiers work | PhD 2019 | Intel Habana → NVIDIA |
| **Itay Hubara** | Binarized Neural Networks — 1-bit weights/activations, AlexNet in 1-bit | MSc → PhD | Habana → Startup (Director AI) |
| **Mor Shpigel Nacson** | Implicit bias of GD — max-margin convergence, step size effects on linear nets | PhD | — |
| **Itay Evron** | Continual learning theory — forgetting rates, POCS framework | PhD 2025 | Meta |
| **Brian Chmiel** | FP4/FP8 LLM training — trillion-token scale, instability in SwiGLU | PhD (w/ Bronstein) | — |

**Conceptual inventory (ideas they operate with):**
- Max-margin solutions (SVM connection)
- Implicit regularization (GD finds simple solutions without explicit L2)
- Loss landscape geometry (flat/sharp minima, stability)
- POCS (projection onto convex sets) for continual learning
- Stochastic rounding as unbiased estimator (quantization)
- Thermodynamics of learning (new 2025 direction)

**What this tells you:** Strong industry placement (NVIDIA, Intel, Meta). Theory-focused but publishable at top venues. Active publication rate suggests hands-on advising. Clear research program with multiple entry points.

**Your fit:** Physics background → loss landscape, Langevin dynamics, optimization as physical system. Linguistics background → applying quantization to multilingual models. His FP4/FP8 work is hottest current direction.

**Source:** [soudry.github.io](https://soudry.github.io/), [publications](https://soudry.github.io/publications/), [team](https://soudry.github.io/team/)

---

### Roy Schwartz (HUJI) ✅ Actively Recruiting

**Research:** Efficient NLP, Green AI, model reliability, vocabulary optimization

**Recent output (2024-2025):**
- "Vocab Diet" (Oct 2025) — vocabulary restructuring
- "Context Length Alone Hurts LLM Performance" (Oct 2025)
- "How Quantization Shapes Bias" (Sep 2025)
- "SpeLLM" (Jul 2025) — character-level decoding
- "Don't Overthink It" (May 2025) — reasoning chain optimization

**Current students:** 5 PhD students (Michael Hassid, Yuval Reif, Amit Ben Artzy, Ido Amos, Guy Kaplan)

**Where alumni went:**
| Name | Degree | Current |
|------|--------|---------|
| Yonatan Bitton | PhD 2023 | Google AI |
| Tamer Ghattas | MSc 2025 | AI21 |
| Matanel Oren | MSc 2025 | Microsoft |
| Daniel Rotem | MSc 2023 | Mobileye |
| Inbal Magar | MSc 2022 | AI21 |

**What this tells you:** Excellent placement at top companies (Google, Microsoft, Mobileye, AI21). MSc students go to good places too — not just PhD. Lab seems productive and well-connected.

**Your fit:** Linguistics background → vocabulary design. Physics background → efficiency optimization. His "Green AI" paper has 3000+ citations — high-impact, well-defined research direction.

**Source:** [schwartz-lab-huji.github.io](https://schwartz-lab-huji.github.io/)

---

### Yoav Goldberg (BIU) ⚠️ High Standards

**Research:** NLP methods, syntactic parsing, neural network analysis, LLM behavior

**What he expects** (from [his note to prospective students](http://u.cs.biu.ac.il/~yogo/note-to-grads.html)):
- Work competitive with Stanford/MIT/CMU
- No outside employment during research
- Strong programming (not just "I know Java")
- Linux proficiency required
- Genuine curiosity about language/data/learning
- No guaranteed timeline — you're done when you're done

**Alumni outcomes** (partial):
| Name | Current |
|------|---------|
| Omer Levy | Meta AI, Tel Aviv |
| Hila Gonen | Postdoc, UW |
| Roee Aharoni | Google Research |
| Gail Weiss | EPFL |

**What this tells you:** Very high bar, but alumni go to top places (Meta AI, Google Research, top universities). Self-directed students thrive. If you need external structure, look elsewhere.

**Your fit:** If you're already self-motivated and have strong programming skills, this could be excellent. If you need hand-holding, avoid.

**Source:** [u.cs.biu.ac.il/~yogo](https://u.cs.biu.ac.il/~yogo/)

---

### Reut Tsarfaty (BIU) — ERC-Funded

**Research:** Hebrew NLP, morphological parsing, natural language programming (NLPRO project)

**Recent output (2024):**
- ACL 2024 plenary talk (Bangkok)
- 2 papers at COLM (with Google, Technion collaborators)
- Eye tracking + NLP research

**Current project:** NLPRO (ERC-funded) — can we program computers in natural language?

**Notable alumni:**
| Name | After PhD | Current |
|------|-----------|---------|
| Valentina Pyatkin | AI2 + UW Postdoc | ACL 2024 Best Theme Paper Award (OLMo) |
| Do June Min | PhD 2024 | Recently defended |

**What this tells you:** ERC funding = resources and prestige. Valentina Pyatkin's trajectory (AI2, ACL award) shows strong mentorship. Smaller lab than Goldberg's but focused.

**Your fit:** Linguistics background → morphological analysis, Hebrew NLP is underexplored with clear contributions to make.

**Source:** [nlp.biu.ac.il/~rtsarfaty](https://nlp.biu.ac.il/~rtsarfaty/)

---

### Yonatan Belinkov (Technion) ⚠️ Not Accepting 2025-26

**Research:** Interpretability, robustness, emergent communication, biological LMs, Hebrew/Arabic NLP

**Status:** Spending 2025-26 at Harvard Kempner Institute. Contact in spring 2026 for 2026-27 start.

**Current students:**
- Yaniv Nikankin (PhD) — mechanistic interpretability across language/vision/biology
- Anja Reusch (Postdoc) — Azrieli Fellow, ICML 2025 workshop on interpretability

**Recent output (2024-2025):**
- ICLR 2025: "Arithmetic Without Algorithms"
- COLM 2024: "Have Faith in Faithfulness" (circuits framework)
- ACL 2024: "Diffusion Lens" (text-to-image interpretability)
- "Backward Lens" — Best paper award

**What this tells you:** Hot area (interpretability), prestigious visiting position (Harvard). Small but focused lab. Worth waiting for if interpretability is your interest.

**Your fit:** If you want to understand how LLMs work internally, this is the place. Physics intuition could help with mechanistic analysis.

**Source:** [belinkov.com](https://belinkov.com/)

---

### Shie Mannor (Technion) — Also at NVIDIA

**Research:** Reinforcement learning, learning theory, Markov decision processes

**Where alumni went:**
| Name | Current |
|------|---------|
| Aviv Tamar | Associate Prof, Technion (now runs Robot Learning Lab) |
| Tom Zahavy | Staff Research Scientist, Google DeepMind |
| Gal Dalal | Sr. Research Scientist, NVIDIA |
| Chen Tessler | Research Scientist, NVIDIA Research |
| Daniel Mankowitz | Ethos |
| Assaf Hallak | NVIDIA Research |
| Dotan Di Castro | Research Manager, Bosch-AI |

**What this tells you:** Exceptional placement — DeepMind, NVIDIA, faculty positions. Strong industry connections through his NVIDIA affiliation. If you want RL + industry career, this is excellent.

**Your fit:** Physics background → RL theory (MDPs are stochastic dynamics). Strong math required.

**Source:** [rlrl.net.technion.ac.il](https://rlrl.net.technion.ac.il/)

---

### Ido Dagan (BIU) — NLP Pioneer

**Research:** Textual entailment, semantic inference, knowledge acquisition

**Notable alumni:**
| Name | Year | Current |
|------|------|---------|
| Jonathan Berant | 2012 | Associate Prof, TAU (major NLP researcher) |
| Omer Levy | 2016 | Meta AI |
| Gabriel Stanovsky | 2018 | HUJI faculty |
| Vered Shwartz | 2019 | UBC faculty |
| Valentina Pyatkin | 2024 | AI2 + UW, ACL award winner |

**What this tells you:** Strong track record of producing faculty and top researchers. Co-advises with Tsarfaty. Founder of RTE (Recognizing Textual Entailment) challenge — influential in NLP.

**Your fit:** If you want semantic understanding, inference, knowledge representation. Established researcher with proven mentorship.

**Source:** [u.cs.biu.ac.il/~dagan](https://u.cs.biu.ac.il/~dagani/)

---

### Amir Globerson (TAU) — NeurIPS Leadership

**Research:** Machine learning, probabilistic inference, vision, NLP

**Position:** NeurIPS 2024 General Chair, also at Google Research

**Notable output:** 5 best paper awards (2 NIPS, 2 UAI, 1 ICML)

**Notable students:**
- Amir Bar (PhD) — TAU + UC Berkeley, now publishing at NeurIPS

**What this tells you:** Very well-connected (NeurIPS chair, Google). High-impact researcher. May be busy with leadership roles.

**Source:** [english.tau.ac.il/profile/gamir](https://english.tau.ac.il/profile/gamir)

---

## Research Proposal: Soudry Lab

*A detailed pitch for Daniel Soudry, based on his group's research trajectory and your background.*

### Proposal: Precision-Aware Training for Multilingual LLMs

**One-liner:** Do multilingual models need different numerical precision in different components, and can linguistic structure guide quantization strategy?

---

#### Motivation

FP4 and FP8 training is emerging as the next frontier in efficient deep learning. Soudry's group has published spotlights at ICLR 2025 ("Scaling FP8 to trillion-token LLMs") and NeurIPS 2025 ("FP4 All the Way"). However, all current work focuses on English-centric models.

Multilingual models present distinct challenges:
- **Morphological complexity:** Hebrew, Arabic, Finnish encode more information per token than English
- **Tokenization variance:** BPE produces vastly different token counts across languages
- **Attention patterns:** Syntactically diverse languages may require different precision in attention vs. FFN layers

**Core hypothesis:** Languages with richer morphology are more sensitive to quantization error. Attention layers handling these languages need higher precision than layers handling English.

---

#### Research Questions

1. **Layer sensitivity:** Which transformer components (attention, FFN, embeddings) are most affected by FP4 quantization in multilingual settings?

2. **Language-specific degradation:** Does perplexity degrade uniformly across languages, or do morphologically complex languages suffer disproportionately?

3. **Linguistics-informed quantization:** Can we use linguistic features (morphological richness, word order flexibility) to predict which layers need higher precision?

4. **Mixed-precision strategies:** What's the Pareto frontier of quality vs. efficiency for heterogeneous precision allocation?

---

#### Proposed Experiments

**Phase 1: Characterization (3 months)**
- Take a pretrained multilingual model (mT5, BLOOM, or Aya)
- Apply uniform FP8 → FP4 quantization
- Measure perplexity by language family: Germanic, Semitic, Finno-Ugric, Slavic
- Identify languages with worst degradation

**Phase 2: Layer Analysis (3 months)**
- Apply layer-wise quantization (FP4 for some layers, FP8 for others)
- Track which configurations preserve performance for which languages
- Build sensitivity profiles per language family

**Phase 3: Linguistic Predictors (3 months)**
- Correlate quantization sensitivity with:
  - Morphological richness (WALS features)
  - Tokenization granularity (tokens per word)
  - Syntactic flexibility (dependency length)
- Develop a heuristic for allocating precision budgets

**Phase 4: Training from Scratch (6 months)**
- Train a small multilingual model (~350M) with heterogeneous precision from the start
- Compare to uniform FP8 and uniform FP4 baselines
- Measure final quality/efficiency tradeoff

---

#### Why This Fits Soudry Lab

| Lab strength | Proposal alignment |
|--------------|-------------------|
| FP4/FP8 training (Chmiel, Fishman) | Direct extension to multilingual |
| Quantization theory (Hubara legacy) | Linguistic structure as new variable |
| LLM-scale experiments | Same scale, new languages |
| Industry connections (Intel, NVIDIA) | Multilingual efficiency = commercial value |

---

#### Your Unique Contribution

| Background | How it helps |
|------------|--------------|
| Physics | Information-theoretic analysis of precision requirements |
| Computational linguistics | Linguistic typology, morphological analysis, tokenization expertise |
| Both | Formalize "information density per token" across languages |

---

#### Potential First Paper

**Title:** "Does Hebrew Need More Bits? Language-Specific Precision Requirements in Multilingual LLMs"

**Venue:** EMNLP 2027 or ICLR 2027

**Contribution:** First systematic study of how linguistic typology affects quantization sensitivity. Practical guidelines for multilingual FP4 training.

---

#### Alternative Angle: Langevin Dynamics

If the quantization angle doesn't resonate, a second proposal leverages the 2025 NeurIPS spotlight on temperature in Langevin dynamics:

**Proposal:** "Thermodynamics of Language Model Training"

Treat LLM training as a statistical mechanical system. The "temperature" (learning rate × batch noise) determines which basins the optimization explores. Questions:
- Do transformers exhibit phase transitions during training?
- Can we predict "grokking" using thermodynamic tools?
- Is there an entropy interpretation of the implicit bias in Adam vs. SGD?

This would be higher-risk but leverages your physics background more directly.

---

#### Competitive Landscape: Who Else Works on This

**Quantization / Low-Precision Training:**

| Group | Institution | Focus | Relation to Soudry |
|-------|-------------|-------|-------------------|
| **Microsoft Research Asia** | Microsoft | FP4 LLM training framework (Jan 2025) | Direct competitor — same problem, same timeline |
| **NVIDIA Research** | NVIDIA | NVFP4 format for Blackwell GPUs | Hardware partner — Soudry alumni work here |
| **DeepSeek-AI** | DeepSeek (China) | FP8 at 671B scale (MoE) | Scale leader — proved FP8 works at extreme scale |
| **IBM Research** | IBM | Accumulation bit-width for ultra-low precision | Complementary — focuses on different bottleneck |
| **Han Lab (MIT)** | MIT | TinyML, efficient LLM inference | Parallel track — more inference-focused |

**Implicit Bias / Deep Learning Theory:**

| Researcher | Institution | Focus | Relation to Soudry |
|------------|-------------|-------|-------------------|
| **Nathan Srebro** | TTI-Chicago / UChicago | Optimization geometry, implicit bias | Close collaborator — many joint papers |
| **Sanjeev Arora** | Princeton | DL theory, RLHF, in-context learning | Senior theorist — complementary focus |
| **Song Mei** | UC Berkeley | Statistical theory of deep learning, CLIP | Rising star — NSF CAREER 2025 |
| **Peter Bartlett** | UC Berkeley / Google DeepMind | Learning theory, generalization bounds | Foundational figure — writes the theory textbooks |
| **Jason Lee** | Princeton | Optimization, implicit bias | Active collaborator with Soudry |

**Multilingual Efficiency:**

| Work | Authors | Finding |
|------|---------|---------|
| "How Does Quantization Affect Multilingual LLMs?" | Ramesh et al. 2024 | French degrades -16.6% at W4; Japanese +7.4% at W8 then -16% at W4 |
| IJCAI 2025 | Various | Task difficulty interacts with quantization and model size |
| EMNLP 2024 Findings | Various | Multilingual fairness concerns in compression |

**Key insight:** The multilingual angle is underexplored. Microsoft and NVIDIA focus on English; Soudry's FP4 work is also English-centric. A linguistics-informed approach would be differentiated.

---

#### Draft Email

```
Subject: PhD inquiry — FP4 training for multilingual LLMs

Dear Prof. Soudry,

I'm [name], a physicist with computational linguistics experience. Your
ICLR 2025 spotlight on FP8 training and the NeurIPS 2025 FP4 work are
exciting — I'm curious whether multilingual models behave differently
under extreme quantization.

Specifically: languages with rich morphology (Hebrew, Arabic, Finnish)
encode more information per token than English. I hypothesize that
attention layers for these languages are more sensitive to FP4
quantization. If true, we could develop linguistically-informed
precision allocation strategies.

I'm also drawn to the Langevin dynamics connection in "Temperature is
All You Need" — training dynamics as a physical system is an unexplored
angle I'd like to develop.

Would you be open to a brief call to discuss potential directions?

Best,
[name]
```

---

## Top 12 Project Ideas (Developed)

*Well-specified problems with clear objectives, established methods, and active advisors. Start here.*

### 1. Vocabulary Optimization for Efficient LLMs

**Advisor:** Roy Schwartz (HUJI) ✅ actively recruiting

**The problem:** LLM inference cost scales with vocabulary size, but vocabulary design is understudied. Schwartz's group has shown optimal vocabularies can reduce compute while maintaining performance.

**Why tractable:**
- Clear metric: tokens/second, energy/query
- Established benchmarks (GLUE, SuperGLUE)
- Active advisor with published methodology

**Research questions:**
1. What is the optimal vocabulary size for different tasks and languages?
2. How do subword tokenizers affect downstream efficiency?
3. Can vocabulary be dynamically adjusted during inference?

**Your fit:** Linguistics background gives intuition for morphological tokenization; physics background enables optimization analysis.

**Next step:** Read [Schwartz Lab publications](https://schwartz-lab-huji.github.io/publications/) and email Roy with specific questions about vocabulary research.

---

### 2. Green AI: Carbon-Aware Training

**Advisor:** Roy Schwartz (HUJI) ✅ actively recruiting

**The problem:** Training large models has significant environmental cost. Schwartz's highly-cited "Green AI" paper (2020) established the research agenda for minimizing compute while maintaining capability.

**Why tractable:**
- Measurable: energy per accuracy point
- Established benchmarks and methodology
- Growing field with industry interest

**Research questions:**
1. How do we measure the true energy cost of NLP systems?
2. Can carbon-aware training schedules reduce environmental impact?
3. What architectural choices minimize compute-per-accuracy?

**Key paper:** [Green AI (Schwartz et al., 2020)](https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext) — 3000+ citations

**Next step:** Read the Green AI paper and recent Schwartz lab work on efficiency.

---

### 3. Optimization Landscape of Deep Learning

**Advisor:** Daniel Soudry (Technion) ✅ actively recruiting

**The problem:** Why do neural networks trained with gradient descent generalize well despite having many more parameters than training examples? The geometry of the loss landscape holds answers.

**Why tractable:**
- Physics-native problem (energy surfaces, critical points)
- Strong theoretical tradition
- Soudry has multiple published results

**Research questions:**
1. What geometric properties predict trainability?
2. How does overparameterization affect landscape structure?
3. Can we design architectures with favorable landscapes?

**Your fit:** Physics background maps directly to loss landscape analysis (think: statistical mechanics of learning).

**Key papers:** Check [Soudry's Google Scholar](https://scholar.google.com/citations?user=soudry) for recent loss landscape work.

---

### 4. Developmental Interpretability

**Advisor:** Daniel Soudry (Technion) ✅ actively recruiting

**The problem:** Most interpretability research studies trained models. This emerging subfield studies how neural network mechanisms **form during training** — like developmental biology for AI.

**Why tractable:**
- Well-defined: track circuit formation over training
- Active advisor with relevant expertise
- Connects to Soudry's work on training dynamics

**Research questions:**
1. What phase transitions occur during training?
2. Can we predict final model behavior from early training dynamics?
3. How do circuits emerge and stabilize?

**Key paper:** [Developmental Interpretability (arXiv 2508.15841)](https://arxiv.org/abs/2508.15841)

---

### 5. Hebrew NLP: Beyond Morpho-Syntactic Tasks

**Advisor:** Reut Tsarfaty (BIU) — ERC-funded NLPRO project

**The problem:** Current Hebrew NLP benchmarks focus on morphology and syntax. Semantic understanding (reading comprehension, inference, QA) is underdeveloped. BIU-NLP is building Hebrew MRC datasets.

**Why tractable:**
- Clear gap in existing benchmarks
- ERC-funded lab with resources
- Your linguistics background is directly relevant

**Research questions:**
1. How can morphological analysis improve semantic task performance in Hebrew?
2. Can cross-lingual transfer from Arabic (another Semitic language) help?
3. What evaluation metrics are appropriate for morphologically-rich languages?

**Key resource:** [BIU-NLP Lab](https://biu-nlp.github.io/)

---

### 6. Morphological Analysis for Low-Resource Languages

**Advisor:** Reut Tsarfaty (BIU)

**The problem:** Low-resource languages with rich morphology (Dogri, Luganda, Hebrew dialects) benefit from hybrid approaches combining rules with neural methods. Recent work shows 35-38% BLEU improvement from morphological features.

**Why tractable:**
- Clear methodology exists
- Measurable improvements (BLEU)
- Builds on linguistics expertise

**Research questions:**
1. Can we design tokenization that respects morphological boundaries?
2. How do morphological features improve translation quality?
3. Can morphological structure guide attention mechanisms?

**Key paper:** [Hybrid morphological analysis (Springer 2025)](https://link.springer.com/article/10.1007/s42979-025-04429-9)

---

### 7. Information Bottleneck in Deep Learning

**Advisor:** Nati Linial (HUJI)

**The problem:** The information bottleneck principle suggests networks compress inputs while preserving task-relevant information. This connects information theory to deep learning generalization.

**Why tractable:**
- Physics-native (information theory)
- Clear theoretical questions
- Strong tradition at HUJI

**Research questions:**
1. Does training actually optimize the information bottleneck?
2. How does architecture affect information flow?
3. Can we design architectures that explicitly optimize information objectives?

**Your fit:** Physics background provides information theory intuition.

---

### 8. Random Matrices and Deep Learning

**Advisors:** Ofer Zeitouni, Boaz Klartag (Weizmann) ✅ recruiting

**The problem:** Random matrix theory provides tools for understanding high-dimensional data and neural network training dynamics. Weizmann has world-class probability faculty.

**Why tractable:**
- Physics background directly relevant
- World-class advisors
- Clear mathematical framework

**Research questions:**
1. What are the universality limits of random matrix eigenvalue statistics?
2. How do random matrices connect to neural network training dynamics?
3. Can random matrix theory explain deep learning phenomena?

**Your fit:** Physics + math background is ideal for this direction.

---

### 9. Desalination Process Optimization

**Institution:** BGU Zuckerberg Institute for Water Research

**The problem:** Israel leads the world in desalination (~70% of domestic water). ML can reduce energy consumption and predict membrane fouling.

**Why tractable:**
- Clear objective: energy per cubic meter
- Real-world data available
- Israel is global leader with industry connections

**Research questions:**
1. Can we predict membrane fouling before it occurs?
2. What control strategies minimize energy consumption?
3. How do we handle varying water quality inputs?

**Your fit:** Physics background enables process modeling; optimization is well-defined.

---

### 10. Climate Modeling with Physics-Informed Neural Networks

**Institution:** Weizmann Earth and Planetary Sciences

**The problem:** Climate models are computationally expensive. Physics-informed neural networks (PINNs) can incorporate conservation laws while learning from data.

**Why tractable:**
- Physics-informed approach is your strength
- Growing field with clear methodology
- Climate relevance ensures funding

**Research questions:**
1. How should conservation laws be encoded in climate models?
2. Can neural networks learn subgrid-scale processes?
3. What uncertainty quantification is achievable?

---

### 11. Code-Switching in Hebrew-English Text

**Advisor:** Reut Tsarfaty (BIU)

**The problem:** Israeli texts freely mix Hebrew and English ("Hebrish"). Code-switching NLP must handle mixed scripts and rapid language alternation.

**Why tractable:**
- Underexplored but practical
- Clear evaluation on social media data
- Builds on morphological expertise

**Research questions:**
1. How should tokenizers handle code-switched text?
2. Can language ID improve downstream NLP tasks?
3. What linguistic patterns govern code-switching in Israeli social media?

---

### 12. Precision Irrigation from Sensor Data

**Institution:** BGU Zuckerberg Institute for Water Research

**The problem:** Water-efficient agriculture is critical in arid regions. ML can optimize irrigation scheduling from soil moisture, weather, and crop models.

**Why tractable:**
- Clear objective: water efficiency
- Israel is world leader in drip irrigation
- Measurable real-world impact

**Research questions:**
1. How should sensor data and weather forecasts be combined?
2. Can we learn crop-specific water needs?
3. What is the optimal sensor density for different farm sizes?

---

## Remaining Project Ideas (Scored)

*73 additional ideas, pruned from original 128. Grouped by theme with tractability score.*

### NLP & Language (18 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 1 | Mechanistic interpretability of LLMs | Medium | Goldberg, Reichart |
| 3 | String counterfactuals for robustness | High | Goldberg, Dagan |
| 4 | Zero-shot NER via type embeddings | High | Goldberg |
| 5 | Few-shot learning: diversity over quantity | Medium | Goldberg, Mansour |
| 47 | Jailbreak attack analysis | Medium | Goldberg, Schwartz |
| 48 | Adversarial robustness in Hebrew/Arabic | High | Tsarfaty, Goldberg |
| 49 | Uncertainty quantification in LLMs | High | Schwartz, Soudry |
| 56 | Palestinian Arabic NLP | Medium | Tsarfaty |
| 57 | Hebrew-Arabic cross-lingual transfer | High | Tsarfaty |
| 58 | Diacritization for Semitic languages | High | Tsarfaty |
| 59 | Neuro-symbolic reasoning | Medium | Dagan, Abend |
| 60 | Compositional generalization | Medium | Abend, Goldberg |
| 61 | Temporal reasoning in NLP | Medium | Dagan |
| 62 | Knowledge graph + LLM integration | Medium | Dagan, Shapira |
| 79 | Clinical NLP for Hebrew | Medium | Tsarfaty |
| 80 | Misinformation detection (multilingual) | Medium | Goldberg, Elovici |
| 81 | Educational NLP assessment | Medium | NLP faculty |
| 82 | Legal NLP for Israeli law | Medium | Dagan |

### ML Theory (12 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 7 | Sample complexity of online RL | Medium | Mannor, Mansour |
| 8 | Adversarially robust PAC learning | Medium | Mansour, Linial |
| 9 | Margin bounds for voting classifiers | Medium | Mansour, Globerson |
| 10 | Kolmogorov complexity and generalization | Medium | Linial, Goldreich |
| 11 | Sample complexity of diffusion models | High | Globerson, Soudry |
| 41 | Efficient training at scale | High | Soudry, Globerson |
| 43 | Continual learning theory | Medium | Mannor, ELSC |
| 70 | Unlearning in LLMs | High | Soudry, Goldberg |
| 74 | Manifold learning for high-D data | Medium | Klartag, Soudry |
| 75 | Information bottleneck | High | Linial, Soudry |
| 76 | MDL and neural networks | Medium | Linial, Mansour |
| 78 | Rate-distortion for generative models | Medium | Globerson, Soudry |

### Mathematics & TCS (8 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 12 | Additive combinatorics + communication | Medium | Ziegler |
| 14 | Metric geometry and algorithms | Medium | Naor |
| 15 | GNNs for combinatorial optimization | High | Kimmel, Ailon |
| 16 | Diffusion for combinatorial optimization | Medium | Mannor, Barequet |
| 36 | Graph algorithms + learning | Medium | Elkin, Linial |
| 37 | LLM reasoning on graphs | Medium | NLP + TCS faculty |
| 38 | Computational geometry + vision | Medium | Kimmel, Barequet |
| 66 | LLM-assisted theorem proving | Low | Math faculty |

### Neuroscience & Cognition (6 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 21 | LLMs as brain models | Medium | Globerson, ELSC |
| 22 | Brain encoding/decoding | Medium | ELSC, NLP faculty |
| 23 | Neuromorphic language models | Low | ELSC, Technion EE |
| 51 | Grounded language learning | Medium | Abend, Wolf |
| 52 | Video understanding | Medium | Irani, Wolf |
| 53 | Audio-visual speech (Semitic) | Medium | ELSC |

### Energy & Climate (8 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 83 | Solar cell materials discovery | Medium | Weizmann Chemistry |
| 86 | Smart grid optimization | High | Mannor |
| 87 | Agricultural yield prediction | Medium | Weizmann Earth Sci |
| 106 | Battery materials discovery | Medium | Technion Materials |
| 107 | Hydrogen fuel cell catalysts | Medium | Weizmann Chemistry |
| 108 | Building energy digital twins | Medium | Technion Civil |
| 109 | Wind farm optimization | Medium | Mannor |
| 110 | Perovskite solar stability | Medium | Weizmann Chemistry |

### Biotech & Chemistry (12 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 88 | Molecular property prediction (GNNs) | High | Kimmel |
| 89 | Protein structure + design | Medium | Weizmann Structural Bio |
| 90 | Retrosynthesis planning | Medium | Weizmann Chemistry |
| 91 | Drug-target interaction | Medium | Goldberg, Weizmann |
| 92 | Single-cell RNA analysis | Medium | Weizmann Bio Regulation |
| 93 | CRISPR guide RNA design | Medium | Harel, Weizmann |
| 94 | Microbiome-host modeling | Medium | Weizmann Immunology |
| 95 | Antibiotic resistance prediction | Medium | Weizmann, BGU |
| 119 | Vaccine target identification | Medium | Weizmann Immunology |
| 121 | Personalized nutrition | Medium | Weizmann (Segal) |
| 127 | Organ-on-chip analysis | Low | Technion Biomedical |
| 128 | Stem cell differentiation | Low | Weizmann MCB |

### Privacy & Security (5 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 31 | Cyber security via NLP | Medium | Elovici, Goldberg |
| 67 | Differentially private NLP | Medium | Mansour, Schwartz |
| 68 | Federated multilingual learning | Medium | Mannor, Mansour |
| 69 | Membership inference attacks | Medium | Elovici |
| 98 | Federated learning for medical imaging | Medium | Mansour |

### Sustainability & Society (4 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 102 | Assistive technology / accessibility | Medium | NLP faculty |
| 103 | Archaeological site detection | Medium | Irani, HUJI Archaeology |
| 104 | Endangered language documentation | Medium | Tsarfaty |
| 105 | Humanitarian earth observation | Medium | Technion EE |

---

## Priority Contact List

Based on recruitment status and research fit:

**Tier 1 — Explicitly Recruiting:**
1. Daniel Soudry (Technion) — daniel.soudry@gmail.com
2. Roy Schwartz (HUJI) — schwartz@cs.huji.ac.il
3. Weizmann Math/CS — Apply through Feinberg School

**Tier 2 — Active Labs:**
4. Reut Tsarfaty (BIU) — Hebrew NLP
5. Amir Globerson (TAU) — ML theory
6. Shie Mannor (Technion) — RL theory
7. Ido Dagan (BIU) — NLP semantics

**Tier 3 — Conditional:**
8. Yoav Goldberg (BIU) — High standards, read his note first
9. Yonatan Belinkov (Technion) — For 2026-27 start only

---

## Funding

| Source | Amount | Notes |
|--------|--------|-------|
| **Weizmann** | Free + stipend | All admitted students |
| **HUJI Mandel** | 75,000 NIS/year | 4 years max |
| **Excellence Fellowship** | 168,000 NIS/year | Competitive |
| **Fulbright Israel** | Full support | US citizens |
| **Azrieli Visiting** | 12,000 NIS/month | Short visits |

---

## Note on Peaceful Research

*All project ideas in this document focus on civilian, peaceful applications. Topics with potential dual-use concerns have been excluded. Research should serve humanitarian goals: health, sustainability, knowledge, and human flourishing.*

---

*For art residencies and senior tech positions, see [Levant Resources](/levant/)*
