---
layout: text
title: Israeli Academia Guide
permalink: /israeli-academia/
---

# Israeli Academia: PhD Programs & Research

*Complete guide to graduate programs in Math, CS, and AI at Israeli universities.*

---

## TL;DR — What To Do Now

**It's January 2026. For Fall 2026 entry:**

| If you want to... | Do this NOW |
|-------------------|-------------|
| **Apply formally** | Email 2-3 advisors this month; deadlines are March-May |
| **Avoid paperwork** | Start a remote collaboration instead (see [low-bureaucracy paths](#if-you-refuse-to-provide-records)) |
| **Just explore** | Read advisor papers, attend online seminars, no commitment |

**Key insight:** The formal application is not the bottleneck. Finding an advisor who wants to work with you is. Everything else follows.

---

## Timelines: When to Engage

*Your first question answered: when do I need to act?*

### Critical Deadlines (Fall 2026 Entry)

| Date | Action | University |
|------|--------|------------|
| **NOW** | Start reading faculty papers, identify 2-4 potential advisors | All |
| **Oct 2025** | Begin emailing advisors (brief, specific) | All |
| **Nov 5, 2025** | Weizmann application opens | Weizmann |
| **Nov-Dec 2025** | Request transcripts, ask recommenders | All |
| **Feb 15, 2026** | TAU housing deadline | TAU |
| **March 28, 2026** | **Weizmann deadline** | Weizmann |
| **March 31, 2026** | **HUJI deadline** | HUJI |
| **April 2026** | Technion deadline | Technion |
| **May 31, 2026** | **TAU deadline** | TAU |
| **May-July 2026** | Interviews, decisions | All |

**Bottom line:** If you want to start Fall 2026, you need to be actively emailing advisors by October 2025 and submitting applications by March 2026.

### Time Investment

| Component | Time | Notes |
|-----------|------|-------|
| Program research | 2-3 weeks | Read faculty publications |
| Statement of purpose | 2-3 weeks | Expect 5+ drafts |
| Research proposal (PhD) | 2-4 weeks | Required for most PhD programs |
| CV preparation | 1 week | Academic format |
| Recommendation letters | 4 weeks lead time | Ask early |
| Application forms | 1 week | Tedious but important |

---

## Credentials: What You'll Need

*Your second question addressed: what records do they require?*

### Typical Requirements

| Document | Required By | Notes |
|----------|-------------|-------|
| **Official transcripts** | All | Usually need official/certified copies |
| **Degree certificate** | All | Or expected completion letter |
| **Statement of purpose** | All | 1-2 pages, program-specific |
| **CV** | All | Academic format with photo (Israel norm) |
| **Recommendation letters** | All | 2-3, academic preferred |
| **TOEFL/IELTS** | Most | If English not native; ~80 iBT / 6.5 IELTS |
| **GRE** | Some | Technion EE/Aero require; others optional |
| **Research proposal** | PhD | 2-5 pages on proposed research |
| **Application fee** | Most | $100-150 (Weizmann: free) |

### If You Refuse to Provide Records

*Reality check: formal PhD programs require documentation. But there are paths with lower bureaucratic burden.*

**Absolute minimums (can't avoid):**
- Some form of identity verification
- Proof you can do the work (publications, code, or demonstrated expertise)
- An advisor willing to vouch for you

**What you can often skip:**
- GRE — Most Israeli programs don't require it (except Technion EE/Aero)
- TOEFL — If you're clearly fluent (interview demonstrates this)
- Official transcripts — Some advisors will advocate for you without seeing grades
- Application fee — Weizmann is free; others may waive with explanation

**Alternative paths with less paperwork:**

| Path | Bureaucracy | How it works |
|------|-------------|--------------|
| **Research visitor** | Low | Email advisor directly, propose 3-6 month visit, no formal admission |
| **Collaboration** | Very low | Co-author papers remotely, build relationship, formalize later |
| **Industry → Academia** | Medium | Join Israeli tech company, publish, transition to research role |
| **Conference networking** | Very low | Present at Israeli venues (ISCOL, Reversim), meet faculty in person |
| **Postdoc** | Medium | If you have PhD elsewhere, postdoc applications are simpler |

**The "advisor-first" strategy:**

1. Find an advisor whose work you genuinely admire
2. Email them with a specific research idea (not "I want to join your lab")
3. If they're interested, they'll tell you what paperwork actually matters
4. Let them navigate the bureaucracy for you

*Many Israeli academics are pragmatic — if you can do the work, paperwork becomes a formality they'll help you handle.*

### If You're Missing Credentials

**No official transcripts available?**
- Contact universities directly — some accept unofficial copies with explanation
- Some Israeli universities are flexible with international applicants

**No academic recommenders?**
- Research supervisors, thesis advisors are preferred
- Industry supervisors acceptable for some programs
- Explain gaps in statement of purpose

**GPA below threshold?**
- Weizmann requires 90+ (Israeli scale) but accepts explanation letters
- Strong statement + advisor pre-approval can overcome GPA issues
- Consider MSc first to establish track record

---

## Quick Comparison: Universities

| University | Deadline | Tuition | Strengths | Recruiting |
|------------|----------|---------|-----------|------------|
| **Weizmann** | March 28 | **Free + stipend** | Math, TCS, theory | ~10 PhD/year |
| **HUJI** | March 31 | $10-30k | NLP (Schwartz ✅), neuroscience | Active |
| **TAU** | May 31 | $10-30k | Applied ML, industry ties | Active |
| **Technion** | April | Varies | Deep learning (Soudry ✅), optimization | Active |
| **BIU** | Varies | $5-12k | NLP (world-class), lowest tuition | High standards |
| **BGU** | Oct 2025 | $10-30k | Cyber security, water | Smaller |

---

## Actively Recruiting Advisors (January 2026)

*These explicitly state they want PhD students — prioritize these.*

| Advisor | University | Field | Source |
|---------|-----------|-------|--------|
| **Daniel Soudry** | Technion EE | Deep learning theory | [soudry.github.io](https://soudry.github.io/) |
| **Roy Schwartz** | HUJI CS | Efficient NLP, Green AI | [schwartz-lab-huji.github.io](https://schwartz-lab-huji.github.io/) |
| **Weizmann Math/CS** | Weizmann | Math, TCS | [weizmann.ac.il/math/join-us](https://www.weizmann.ac.il/math/join-us) |

### Conditional / Contact Carefully

| Advisor | Status |
|---------|--------|
| **Yonatan Belinkov (Technion)** | Not accepting 2025-26 (sabbatical); **open for 2026-27** |
| **Yoav Goldberg (BIU)** | [Note to prospective students](http://u.cs.biu.ac.il/~yogo/note-to-grads.html) — high standards |

---

## Advisor Deep Dives

*What their students' careers tell us about working with them.*

### Daniel Soudry (Technion) ✅ Actively Recruiting

**Research:** Neural network theory, quantization, optimization dynamics, generalization

**Lab metrics:**
| Metric | Value |
|--------|-------|
| Papers 2023-2025 | 25+ (NeurIPS, ICML, ICLR) |
| Spotlights | 6 (exceptional acceptance rate) |
| Current PhDs | 3 |
| Current MSc | 6 |
| Alumni PhDs | 10 |
| Funding | ERC A-B-C-Deep, Intel (5 grants), ISF |

**Recent output (2024-2025):**
- NeurIPS 2025: 6 papers (2 spotlights) — "Temperature is All You Need for Generalization in Langevin Dynamics", "FP4 All the Way: Fully Quantized Training of LLMs"
- ICLR 2025 spotlight: "Scaling FP8 training to trillion-token LLMs"
- ICML 2025: "When Diffusion Models Memorize..."
- ALT 2026: 2 papers on continual learning
- Member of Israel Young Academy

**Research pillars (from 25 papers):**

| Pillar | Focus | Key People |
|--------|-------|------------|
| **Quantization** | FP4, FP8, binarized networks | Brian Chmiel, Maxim Fishman, Itay Hubara |
| **Implicit bias** | Why GD finds "good" solutions | Mor Shpigel Nacson (alum) |
| **Continual learning** | Catastrophic forgetting theory | Itay Evron (alum, now Meta) |
| **Generalization** | Why overparameterized nets work | Itamar Harel, Gon Buzaglo |
| **Architecture** | Alias-free networks, shift invariance | Hagay Michaeli |

**Current students:** Matan Haroush, Hagay Michaeli, Mikey Shechter (PhD); Gil Denekamp, Itay Lamprecht, Ran Levinstein, Matan Tsipori, Ido Blayberg, Gilad Karpel (MSc)

**Alumni thesis summaries:**

| Name | Thesis/Focus | Duration | Where Now |
|------|--------------|----------|-----------|
| **Elad Hoffer** | "Deep Learning: Rethinking Common Practices" — batch size ≠ generalization, fixed classifiers work | PhD 2019 | Intel Habana → NVIDIA |
| **Itay Hubara** | Binarized Neural Networks — 1-bit weights/activations, AlexNet in 1-bit | MSc → PhD | Habana → Startup (Director AI) |
| **Mor Shpigel Nacson** | Implicit bias of GD — max-margin convergence, step size effects on linear nets | PhD | — |
| **Itay Evron** | Continual learning theory — forgetting rates, POCS framework | PhD 2025 | Meta |
| **Brian Chmiel** | FP4/FP8 LLM training — trillion-token scale, instability in SwiGLU | PhD (w/ Bronstein) | — |

**Conceptual inventory (ideas they operate with):**
- Max-margin solutions (SVM connection)
- Implicit regularization (GD finds simple solutions without explicit L2)
- Loss landscape geometry (flat/sharp minima, stability)
- POCS (projection onto convex sets) for continual learning
- Stochastic rounding as unbiased estimator (quantization)
- Thermodynamics of learning (new 2025 direction)

**What this tells you:** Strong industry placement (NVIDIA, Intel, Meta). Theory-focused but publishable at top venues. Active publication rate suggests hands-on advising. Clear research program with multiple entry points.

**Your fit:** Physics background → loss landscape, Langevin dynamics, optimization as physical system. Linguistics background → applying quantization to multilingual models. His FP4/FP8 work is hottest current direction.

**Source:** [soudry.github.io](https://soudry.github.io/), [publications](https://soudry.github.io/publications/), [team](https://soudry.github.io/team/)

---

### Roy Schwartz (HUJI) ✅ Actively Recruiting

**Research:** Efficient NLP, Green AI, model reliability, vocabulary optimization

**Recent output (2024-2025):**
- "Vocab Diet" (Oct 2025) — vocabulary restructuring
- "Context Length Alone Hurts LLM Performance" (Oct 2025)
- "How Quantization Shapes Bias" (Sep 2025)
- "SpeLLM" (Jul 2025) — character-level decoding
- "Don't Overthink It" (May 2025) — reasoning chain optimization

**Current students:** 5 PhD students (Michael Hassid, Yuval Reif, Amit Ben Artzy, Ido Amos, Guy Kaplan)

**Where alumni went:**
| Name | Degree | Current |
|------|--------|---------|
| Yonatan Bitton | PhD 2023 | Google AI |
| Tamer Ghattas | MSc 2025 | AI21 |
| Matanel Oren | MSc 2025 | Microsoft |
| Daniel Rotem | MSc 2023 | Mobileye |
| Inbal Magar | MSc 2022 | AI21 |

**What this tells you:** Excellent placement at top companies (Google, Microsoft, Mobileye, AI21). MSc students go to good places too — not just PhD. Lab seems productive and well-connected.

**Your fit:** Linguistics background → vocabulary design. Physics background → efficiency optimization. His "Green AI" paper has 3000+ citations — high-impact, well-defined research direction.

**Source:** [schwartz-lab-huji.github.io](https://schwartz-lab-huji.github.io/)

---

### Yoav Goldberg (BIU) ⚠️ High Standards

**Research:** NLP methods, syntactic parsing, neural network analysis, LLM behavior

**What he expects** (from [his note to prospective students](http://u.cs.biu.ac.il/~yogo/note-to-grads.html)):
- Work competitive with Stanford/MIT/CMU
- No outside employment during research
- Strong programming (not just "I know Java")
- Linux proficiency required
- Genuine curiosity about language/data/learning
- No guaranteed timeline — you're done when you're done

**Alumni outcomes** (partial):
| Name | Current |
|------|---------|
| Omer Levy | Meta AI, Tel Aviv |
| Hila Gonen | Postdoc, UW |
| Roee Aharoni | Google Research |
| Gail Weiss | EPFL |

**What this tells you:** Very high bar, but alumni go to top places (Meta AI, Google Research, top universities). Self-directed students thrive. If you need external structure, look elsewhere.

**Your fit:** If you're already self-motivated and have strong programming skills, this could be excellent. If you need hand-holding, avoid.

**Source:** [u.cs.biu.ac.il/~yogo](https://u.cs.biu.ac.il/~yogo/)

---

### Reut Tsarfaty (BIU) — ERC-Funded

**Research:** Hebrew NLP, morphological parsing, natural language programming (NLPRO project)

**Recent output (2024):**
- ACL 2024 plenary talk (Bangkok)
- 2 papers at COLM (with Google, Technion collaborators)
- Eye tracking + NLP research

**Current project:** NLPRO (ERC-funded) — can we program computers in natural language?

**Notable alumni:**
| Name | After PhD | Current |
|------|-----------|---------|
| Valentina Pyatkin | AI2 + UW Postdoc | ACL 2024 Best Theme Paper Award (OLMo) |
| Do June Min | PhD 2024 | Recently defended |

**What this tells you:** ERC funding = resources and prestige. Valentina Pyatkin's trajectory (AI2, ACL award) shows strong mentorship. Smaller lab than Goldberg's but focused.

**Your fit:** Linguistics background → morphological analysis, Hebrew NLP is underexplored with clear contributions to make.

**Source:** [nlp.biu.ac.il/~rtsarfaty](https://nlp.biu.ac.il/~rtsarfaty/)

---

### Yonatan Belinkov (Technion) ⚠️ Not Accepting 2025-26

**Research:** Interpretability, robustness, emergent communication, biological LMs, Hebrew/Arabic NLP

**Status:** Spending 2025-26 at Harvard Kempner Institute. Contact in spring 2026 for 2026-27 start.

**Current students:**
- Yaniv Nikankin (PhD) — mechanistic interpretability across language/vision/biology
- Anja Reusch (Postdoc) — Azrieli Fellow, ICML 2025 workshop on interpretability

**Recent output (2024-2025):**
- ICLR 2025: "Arithmetic Without Algorithms"
- COLM 2024: "Have Faith in Faithfulness" (circuits framework)
- ACL 2024: "Diffusion Lens" (text-to-image interpretability)
- "Backward Lens" — Best paper award

**What this tells you:** Hot area (interpretability), prestigious visiting position (Harvard). Small but focused lab. Worth waiting for if interpretability is your interest.

**Your fit:** If you want to understand how LLMs work internally, this is the place. Physics intuition could help with mechanistic analysis.

**Source:** [belinkov.com](https://belinkov.com/)

---

### Shie Mannor (Technion) — Also at NVIDIA

**Research:** Reinforcement learning, learning theory, Markov decision processes

**Where alumni went:**
| Name | Current |
|------|---------|
| Aviv Tamar | Associate Prof, Technion (now runs Robot Learning Lab) |
| Tom Zahavy | Staff Research Scientist, Google DeepMind |
| Gal Dalal | Sr. Research Scientist, NVIDIA |
| Chen Tessler | Research Scientist, NVIDIA Research |
| Daniel Mankowitz | Ethos |
| Assaf Hallak | NVIDIA Research |
| Dotan Di Castro | Research Manager, Bosch-AI |

**What this tells you:** Exceptional placement — DeepMind, NVIDIA, faculty positions. Strong industry connections through his NVIDIA affiliation. If you want RL + industry career, this is excellent.

**Your fit:** Physics background → RL theory (MDPs are stochastic dynamics). Strong math required.

**Source:** [rlrl.net.technion.ac.il](https://rlrl.net.technion.ac.il/)

---

### Ido Dagan (BIU) — NLP Pioneer

**Research:** Textual entailment, semantic inference, knowledge acquisition

**Notable alumni:**
| Name | Year | Current |
|------|------|---------|
| Jonathan Berant | 2012 | Associate Prof, TAU (major NLP researcher) |
| Omer Levy | 2016 | Meta AI |
| Gabriel Stanovsky | 2018 | HUJI faculty |
| Vered Shwartz | 2019 | UBC faculty |
| Valentina Pyatkin | 2024 | AI2 + UW, ACL award winner |

**What this tells you:** Strong track record of producing faculty and top researchers. Co-advises with Tsarfaty. Founder of RTE (Recognizing Textual Entailment) challenge — influential in NLP.

**Your fit:** If you want semantic understanding, inference, knowledge representation. Established researcher with proven mentorship.

**Source:** [u.cs.biu.ac.il/~dagan](https://u.cs.biu.ac.il/~dagani/)

---

### Amir Globerson (TAU) — NeurIPS Leadership

**Research:** Machine learning, probabilistic inference, vision, NLP

**Position:** NeurIPS 2024 General Chair, also at Google Research

**Notable output:** 5 best paper awards (2 NIPS, 2 UAI, 1 ICML)

**Notable students:**
- Amir Bar (PhD) — TAU + UC Berkeley, now publishing at NeurIPS

**What this tells you:** Very well-connected (NeurIPS chair, Google). High-impact researcher. May be busy with leadership roles.

**Source:** [english.tau.ac.il/profile/gamir](https://english.tau.ac.il/profile/gamir)

---

## Research Proposal: Soudry Lab

*A detailed pitch for Daniel Soudry, based on his group's research trajectory and your background.*

### Proposal: Precision-Aware Training for Multilingual LLMs

**One-liner:** Do multilingual models need different numerical precision in different components, and can linguistic structure guide quantization strategy?

---

#### Motivation

FP4 and FP8 training is emerging as the next frontier in efficient deep learning. Soudry's group has published spotlights at ICLR 2025 ("Scaling FP8 to trillion-token LLMs") and NeurIPS 2025 ("FP4 All the Way"). However, all current work focuses on English-centric models.

Multilingual models present distinct challenges:
- **Morphological complexity:** Hebrew, Arabic, Finnish encode more information per token than English
- **Tokenization variance:** BPE produces vastly different token counts across languages
- **Attention patterns:** Syntactically diverse languages may require different precision in attention vs. FFN layers

**Core hypothesis:** Languages with richer morphology are more sensitive to quantization error. Attention layers handling these languages need higher precision than layers handling English.

---

#### Research Questions

1. **Layer sensitivity:** Which transformer components (attention, FFN, embeddings) are most affected by FP4 quantization in multilingual settings?

2. **Language-specific degradation:** Does perplexity degrade uniformly across languages, or do morphologically complex languages suffer disproportionately?

3. **Linguistics-informed quantization:** Can we use linguistic features (morphological richness, word order flexibility) to predict which layers need higher precision?

4. **Mixed-precision strategies:** What's the Pareto frontier of quality vs. efficiency for heterogeneous precision allocation?

---

#### Proposed Experiments

**Phase 1: Characterization (3 months)**
- Take a pretrained multilingual model (mT5, BLOOM, or Aya)
- Apply uniform FP8 → FP4 quantization
- Measure perplexity by language family: Germanic, Semitic, Finno-Ugric, Slavic
- Identify languages with worst degradation

**Phase 2: Layer Analysis (3 months)**
- Apply layer-wise quantization (FP4 for some layers, FP8 for others)
- Track which configurations preserve performance for which languages
- Build sensitivity profiles per language family

**Phase 3: Linguistic Predictors (3 months)**
- Correlate quantization sensitivity with:
  - Morphological richness (WALS features)
  - Tokenization granularity (tokens per word)
  - Syntactic flexibility (dependency length)
- Develop a heuristic for allocating precision budgets

**Phase 4: Training from Scratch (6 months)**
- Train a small multilingual model (~350M) with heterogeneous precision from the start
- Compare to uniform FP8 and uniform FP4 baselines
- Measure final quality/efficiency tradeoff

---

#### Why This Fits Soudry Lab

| Lab strength | Proposal alignment |
|--------------|-------------------|
| FP4/FP8 training (Chmiel, Fishman) | Direct extension to multilingual |
| Quantization theory (Hubara legacy) | Linguistic structure as new variable |
| LLM-scale experiments | Same scale, new languages |
| Industry connections (Intel, NVIDIA) | Multilingual efficiency = commercial value |

---

#### Your Unique Contribution

| Background | How it helps |
|------------|--------------|
| Physics | Information-theoretic analysis of precision requirements |
| Computational linguistics | Linguistic typology, morphological analysis, tokenization expertise |
| Both | Formalize "information density per token" across languages |

---

#### Potential First Paper

**Title:** "Does Hebrew Need More Bits? Language-Specific Precision Requirements in Multilingual LLMs"

**Venue:** EMNLP 2027 or ICLR 2027

**Contribution:** First systematic study of how linguistic typology affects quantization sensitivity. Practical guidelines for multilingual FP4 training.

---

#### Alternative Angle: Langevin Dynamics

If the quantization angle doesn't resonate, a second proposal leverages the 2025 NeurIPS spotlight on temperature in Langevin dynamics:

**Proposal:** "Thermodynamics of Language Model Training"

Treat LLM training as a statistical mechanical system. The "temperature" (learning rate × batch noise) determines which basins the optimization explores. Questions:
- Do transformers exhibit phase transitions during training?
- Can we predict "grokking" using thermodynamic tools?
- Is there an entropy interpretation of the implicit bias in Adam vs. SGD?

This would be higher-risk but leverages your physics background more directly.

---

#### Competitive Landscape: Who Else Works on This

**Quantization / Low-Precision Training:**

| Group | Institution | Focus | Relation to Soudry |
|-------|-------------|-------|-------------------|
| **Microsoft Research Asia** | Microsoft | FP4 LLM training framework (Jan 2025) | Direct competitor — same problem, same timeline |
| **NVIDIA Research** | NVIDIA | NVFP4 format for Blackwell GPUs | Hardware partner — Soudry alumni work here |
| **DeepSeek-AI** | DeepSeek (China) | FP8 at 671B scale (MoE) | Scale leader — proved FP8 works at extreme scale |
| **IBM Research** | IBM | Accumulation bit-width for ultra-low precision | Complementary — focuses on different bottleneck |
| **Han Lab (MIT)** | MIT | TinyML, efficient LLM inference | Parallel track — more inference-focused |

**Implicit Bias / Deep Learning Theory:**

| Researcher | Institution | Focus | Relation to Soudry |
|------------|-------------|-------|-------------------|
| **Nathan Srebro** | TTI-Chicago / UChicago | Optimization geometry, implicit bias | Close collaborator — many joint papers |
| **Sanjeev Arora** | Princeton | DL theory, RLHF, in-context learning | Senior theorist — complementary focus |
| **Song Mei** | UC Berkeley | Statistical theory of deep learning, CLIP | Rising star — NSF CAREER 2025 |
| **Peter Bartlett** | UC Berkeley / Google DeepMind | Learning theory, generalization bounds | Foundational figure — writes the theory textbooks |
| **Jason Lee** | Princeton | Optimization, implicit bias | Active collaborator with Soudry |

**Multilingual Efficiency:**

| Work | Authors | Finding |
|------|---------|---------|
| "How Does Quantization Affect Multilingual LLMs?" | Ramesh et al. 2024 | French degrades -16.6% at W4; Japanese +7.4% at W8 then -16% at W4 |
| IJCAI 2025 | Various | Task difficulty interacts with quantization and model size |
| EMNLP 2024 Findings | Various | Multilingual fairness concerns in compression |

**Key insight:** The multilingual angle is underexplored. Microsoft and NVIDIA focus on English; Soudry's FP4 work is also English-centric. A linguistics-informed approach would be differentiated.

---

#### Draft Email

```
Subject: PhD inquiry — FP4 training for multilingual LLMs

Dear Prof. Soudry,

I'm [name], a physicist with computational linguistics experience. Your
ICLR 2025 spotlight on FP8 training and the NeurIPS 2025 FP4 work are
exciting — I'm curious whether multilingual models behave differently
under extreme quantization.

Specifically: languages with rich morphology (Hebrew, Arabic, Finnish)
encode more information per token than English. I hypothesize that
attention layers for these languages are more sensitive to FP4
quantization. If true, we could develop linguistically-informed
precision allocation strategies.

I'm also drawn to the Langevin dynamics connection in "Temperature is
All You Need" — training dynamics as a physical system is an unexplored
angle I'd like to develop.

Would you be open to a brief call to discuss potential directions?

Best,
[name]
```

---

## Multilingual Quantization: Hypotheses & Experiments

*A concrete research plan to test whether linguistic typology predicts quantization sensitivity.*

---

### Part 0: Reasoning Shape — Epistemology of Artifact Probing

Before diving into data, we formalize the reasoning structure itself.

#### The Problem

We have a "muddled entity" — the landscape of quantized multilingual models — that contains latent structure. We want to:
1. Discover what structure exists
2. Formulate hypotheses about it
3. Test those hypotheses by eliciting behavior
4. Refine understanding

This is **behavioral epistemology for ML artifacts**: treating existing models as objects of scientific inquiry.

---

#### One: The Universe of Evidential Results

**What exists that we can observe?**

| Category | Artifacts | Observable Properties |
|----------|-----------|----------------------|
| **Models** | BLOOM, Aya, Qwen, mGPT, Command R, Llama, Mistral | Architecture, size, training data, languages |
| **Quantized variants** | GPTQ, AWQ, GGUF, bitsandbytes versions | Bit-width, quantization method, calibration data |
| **Benchmarks** | MMLU, mMMLU, FLORES, MGSM, XCOPA | Per-language scores, before/after quantization |
| **Tokenizers** | SentencePiece, BPE, Unigram | Vocabulary, fertility per language |
| **Linguistic databases** | WALS, Ethnologue, Glottolog | Typological features, genealogy, scripts |
| **Existing studies** | Marchisio et al., tokenization papers | Published degradation numbers |

**Evidential dimensions:**
- Model × Language × Task × Quantization → Performance
- Language → Typological features
- Language × Tokenizer → Fertility
- Layer × Language → Sensitivity (if probed)

---

#### Two: Hypothesis Formulation — The Shape of Conjecture

A well-formed hypothesis for artifact probing has:

```
IF [latent structure X exists in the artifact]
THEN [observable behavior Y should manifest]
WHEN [we apply elicitation procedure Z]
```

**Example (H1):**
```
IF morphologically complex languages have higher-entropy representations
THEN quantization (which reduces entropy capacity) should degrade them more
WHEN we measure perplexity before/after quantization
```

**Key requirement:** The observable behavior must be:
1. **Discriminative** — Different latent structures produce different observables
2. **Accessible** — We can actually measure it without impossible resources
3. **Interpretable** — We can map back from observation to structure

---

#### Three: Elicitation Protocol — Probing the Muddled Entity

Three complementary approaches:

**A. Behavioral Testing (CheckList-style)**
- Define capability (e.g., "handles morphological agreement")
- Design minimal test cases
- Measure pass/fail rate before/after quantization
- Compute degradation by capability type

**B. Probing Classifiers (Belinkov-style)**
- Extract representations from model layers
- Train classifier to predict linguistic property
- Compare probing accuracy: FP16 vs. quantized
- If accuracy drops, that information was "lost to compression"

**C. Correlational Analysis (Epidemiological)**
- Gather cross-model, cross-language data
- Compute correlations with linguistic features
- Use regression to disentangle predictors
- Identify residuals — what remains unexplained?

---

#### Four: Interpretation Framework — What Counts as Evidence?

| Observation | Interpretation | Confidence |
|-------------|----------------|------------|
| Strong correlation (r > 0.7) between morphological complexity and degradation | Morphology drives sensitivity | Medium-high (could be confounded) |
| Correlation disappears when controlling for fertility | Fertility is the true driver, morphology is proxy | High |
| Different layers show different sensitivity patterns by language | Language-specific information is localized | High |
| Probing accuracy drops for morphology but not syntax | Morphological information specifically lost | High |
| Random variation, no pattern | Our hypothesis is wrong OR our measurements are noisy | Requires refinement |

**Falsification conditions:**
- H1 falsified if: No correlation between WALS complexity and degradation
- H2 falsified if: Fertility uncorrelated with degradation
- H3 falsified if: All layers equally sensitive for all languages

---

#### Five: Iteration — Refinement Protocol

```
┌─────────────────────────────────────────┐
│ 1. Survey → What artifacts exist?       │
│ 2. Hypothesize → What structure expect? │
│ 3. Design → What elicits that behavior? │
│ 4. Observe → Run probes, collect data   │
│ 5. Interpret → Does it confirm/falsify? │
│ 6. Refine → New hypotheses, repeat      │
└─────────────────────────────────────────┘
```

**Stopping conditions:**
- Sufficient predictive power (R² > 0.6)
- Clear mechanistic story (layer-level understanding)
- Actionable output (precision allocation heuristics)

---

#### Six: Scale Ladder — From Cheap to Expensive

| Scale | Method | Cost | What it tells us |
|-------|--------|------|------------------|
| **0. Meta-analysis** | Re-analyze published results | Free | Whether hypotheses worth testing |
| **1. Tokenizer probing** | Compute fertility on corpora | Minutes | Fertility as predictor |
| **2. Small model probing** | Run existing quantized small models | Hours | Whether patterns hold |
| **3. Medium model probing** | Run 7B-13B quantized models | Days | Scale invariance |
| **4. Layer-wise analysis** | Extract activations, probe | Days | Mechanistic understanding |
| **5. Controlled training** | Train models with manipulated variables | Weeks | Causal claims |

**Principle:** Don't advance to Scale N+1 until Scale N confirms the pattern.

---

#### Seven: The Instrumentalization Thesis

**Key insight:** The solution to our research problem may already exist — scattered across HuggingFace model cards, academic appendices, community benchmarks, and practitioners' reports. We don't need to discover it; we need to **instrumentalize** it.

**What "instrumentalize" means:**
1. Build tools that aggregate dispersed evidence
2. Create formal mappings between observables and theory
3. Validate that the pattern is real, not noise
4. Package it as actionable knowledge

**The archaeological metaphor:**
- Artifacts exist (quantized models, benchmark scores)
- Evidence exists (degradation patterns, fertility data)
- The structure exists (linguistic typology → quantization sensitivity)
- We're not discovering — we're **excavating and reconstructing**

**Concrete instruments we need:**

| Instrument | What it does | Output |
|------------|--------------|--------|
| **Benchmark aggregator** | Collects per-language scores across models/papers | Language × Model × Quantization → Score matrix |
| **Fertility calculator** | Tokenizes corpora, computes tokens/word | Language × Tokenizer → Fertility |
| **WALS interface** | Maps languages to typological features | Language → Feature vector |
| **Correlation engine** | Computes partial correlations, regressions | Feature → Degradation relationships |
| **Visualization dashboard** | Renders patterns interpretably | Interactive exploration |

**What we expect to find:**
- The pattern is already there: morphologically complex languages degrade more
- It's already in the Marchisio data, the tokenization studies, the community reports
- We just haven't connected the dots formally

**What constitutes success:**
- Not: "We ran experiments and found X"
- But: "We built an instrument that reveals the existing evidence shows X"

**The validation requirement:**
- Our instrument should **predict** benchmark scores we haven't looked at yet
- If we build the model from Marchisio data, it should predict degradation on Aya data we held out
- This is the test that our formalization captures real structure, not overfitting to noise

---

#### Eight: Catalog of Potentially Solved Questions

Questions that may already have answers in existing data:

| Question | Where evidence likely lives | Instrument needed |
|----------|----------------------------|-------------------|
| Does morphology predict degradation? | Marchisio et al. + WALS | Correlation analysis |
| Does fertility predict degradation? | Tokenization papers + model cards | Regression on fertility |
| Which layers are sensitive? | Probing literature + AWQ salient weight analysis | Layer-wise aggregation |
| Does script type matter? | Marchisio non-Latin vs Latin split | Categorical regression |
| Does model size modulate effects? | Across-scale benchmark comparisons | Size × Language interaction |
| Do different quantization methods affect languages differently? | Intel leaderboard + model cards | Method × Language matrix |

**What's genuinely unknown (requires new experiments):**
- Causal mechanisms (why not just what)
- Layer-specific language interactions (fine-grained)
- Whether morpheme-aware tokenization helps (interventional)

---

#### Nine: The Hypothesis Reformulation Loop

A hypothesis is not static. It transforms as we gather evidence and shift perspective.

**The Loop:**
```
┌─────────────────────────────────────────────────────────┐
│ H(n) = current hypothesis                               │
│ C(n) = current context (what we know, what we can do)   │
│ A(n) = actionable perspective (what experiments feasible)│
│                                                         │
│ OBSERVE: Run probes affordable under A(n)               │
│ UPDATE: Evidence E modifies confidence in H(n)          │
│ REFORMULATE: H(n+1) = refine(H(n), E, C(n+1))          │
│ SHIFT: New evidence changes what's actionable → A(n+1)  │
│ REPEAT                                                  │
└─────────────────────────────────────────────────────────┘
```

**Example trajectory:**

| Iteration | Hypothesis | Context | Actionable | Result |
|-----------|------------|---------|------------|--------|
| n=0 | "Morphology predicts degradation" | Have Marchisio data | Re-analyze their tables | r=0.4, weak |
| n=1 | "Fertility mediates the effect" | Computed fertility for 30 langs | Add fertility to regression | r=0.65, partial mediation |
| n=2 | "Fertility + script type jointly predict" | Added script categorical | Multiple regression | R²=0.72, good fit |
| n=3 | "Effect strongest in layers 12-18" | Can now justify layer probing | Run probing on Aya-8B | Confirmed: middle layers |
| n=4 | "Quantization threshold varies by fertility" | Have layer × fertility data | Fit threshold function | τ(fertility) = 4.2 - 0.3*fertility |

**Key properties:**
- Each iteration is cheaper than starting fresh
- Evidence accumulates across iterations
- Hypotheses become more specific and predictive
- Actionability expands as confidence grows

---

#### Ten: Expected Instrumental Results — Quantitative Predictions

The goal is not "correlation exists" but **specific numerical predictions** that can be tested.

**Form of a prediction:**
```
GIVEN: Language L with properties P = {fertility=f, complexity=c, script=s}
       Model M with properties Q = {size=n, architecture=a}
       Quantization method Z at bit-width b

PREDICT: Performance degradation Δ = g(P, Q, Z, b)
         With confidence interval CI
         And threshold b* where degradation becomes unacceptable
```

**Concrete example predictions (to be validated):**

| Language | Fertility | Complexity | Predicted Δ at W4 | Threshold b* | Confidence |
|----------|-----------|------------|-------------------|--------------|------------|
| English | 1.2 | -12 | -2% ± 1% | 3.2 bits | High (baseline) |
| German | 1.8 | -8 | -5% ± 2% | 3.8 bits | Medium |
| Hebrew | 2.1 | -4 | -8% ± 3% | 4.2 bits | Medium |
| Arabic | 3.0 | -3 | -12% ± 4% | 4.5 bits | Medium |
| Finnish | 2.5 | -2 | -10% ± 3% | 4.3 bits | Medium |
| Turkish | 2.3 | -1 | -11% ± 4% | 4.4 bits | Medium |
| Japanese | 2.5 | -6 | -9% ± 3% | 4.1 bits | Low (script confound) |

**The threshold function (hypothesized form):**
```
b*(L) = b_base + α·fertility(L) + β·complexity(L) + γ·script_penalty(L)

Where:
  b_base ≈ 3.2 (English baseline threshold)
  α ≈ 0.3-0.5 (fertility coefficient)
  β ≈ 0.05-0.1 (complexity coefficient)
  γ ≈ 0.3-0.5 (non-Latin script penalty)
```

**Interpretation:**
- English needs only 3.2 bits before quality degrades
- Arabic needs 4.5 bits for equivalent quality
- This is ~40% more precision required

---

#### Eleven: Weight Set "Clumpiness" and Network Properties

Beyond linguistic features, the model itself has measurable properties that affect quantization sensitivity.

**Weight distribution properties:**

| Property | Definition | How it affects quantization |
|----------|------------|----------------------------|
| **Kurtosis** | "Peakedness" of weight distribution | High kurtosis → outliers → needs more bits |
| **Outlier ratio** | % of weights > 3σ from mean | More outliers → worse quantization |
| **Layer entropy** | H(weights) per layer | Higher entropy → more information → more bits needed |
| **Activation range** | max/min ratio of activations | Wider range → harder to quantize |
| **Gradient variance** | Var(∂L/∂w) during training | High variance → unstable under quantization |

**The "clumpiness" concept:**
- Weights cluster non-uniformly across the representational space
- Some regions are dense (well-learned, stable)
- Some regions are sparse (rare features, unstable)
- Quantization affects sparse regions more

**Observable proxies:**
```
clumpiness(layer) = kurtosis(W) × outlier_ratio(W) × (1 / effective_rank(W))
```

**Hypothesis:** Languages with lower training frequency have representations in "sparse" (clumpy) regions, hence more sensitive to quantization.

**Testable prediction:**
```
Δ(language) ∝ clumpiness(language_specific_subspace)
```

If we can identify which parts of the weight space encode which languages (via probing), we can measure clumpiness per language and predict degradation.

---

#### Twelve: The Instrumental Results Table

What specific numbers do we expect our instruments to produce?

**Instrument 1: Benchmark Aggregator**
| Output | Expected range | What it tells us |
|--------|----------------|------------------|
| Δ(English, W4) | -1% to -3% | Baseline degradation |
| Δ(Arabic, W4) | -8% to -15% | High-morphology degradation |
| Δ(Japanese, W4) | -5% to -12% | Script + morphology interaction |
| Correlation(fertility, Δ) | 0.5 to 0.8 | Fertility as predictor |
| Correlation(WALS_complexity, Δ) | 0.3 to 0.6 | Morphology as predictor |

**Instrument 2: Fertility Calculator**
| Output | Expected range | What it tells us |
|--------|----------------|------------------|
| fertility(English) | 1.1 to 1.4 | Baseline |
| fertility(Hebrew) | 1.8 to 2.4 | Moderate inflation |
| fertility(Arabic) | 2.5 to 3.5 | High inflation |
| fertility(Telugu) | 8.0 to 12.0 | Extreme inflation |
| Variance across models | 10-20% | Tokenizer quality matters |

**Instrument 3: Threshold Estimator**
| Output | Expected range | What it tells us |
|--------|----------------|------------------|
| b*(English) | 3.0 to 3.5 bits | English is easy |
| b*(Arabic) | 4.2 to 5.0 bits | Arabic needs more precision |
| b*(Finnish) | 4.0 to 4.8 bits | Agglutinative penalty |
| Slope(b* vs fertility) | 0.2 to 0.5 | Strength of fertility effect |

**Instrument 4: Layer Sensitivity Profiler**
| Output | Expected pattern | What it tells us |
|--------|------------------|------------------|
| Sensitivity(layers 0-6) | Low, uniform | Early layers language-agnostic |
| Sensitivity(layers 12-18) | High, language-varying | Middle layers encode language-specific |
| Sensitivity(final layers) | Medium, task-specific | Late layers more about task |
| Language × Layer interaction | Significant | Different languages in different layers |

---

#### Thirteen: Decision Points — When to Stop, Pivot, or Proceed

**Stop conditions:**
- R² > 0.7 and predictions validate on held-out data → Write paper
- All hypotheses falsified, no pattern → Abandon this direction
- Found but trivial (e.g., just training data size) → Reframe contribution

**Pivot conditions:**
- Fertility dominates, morphology irrelevant → Focus on tokenization research
- Layer effects dominate linguistic features → Focus on architecture analysis
- Model size swamps all other effects → Focus on scaling laws

**Proceed conditions:**
- Preliminary correlation confirmed → Invest in larger-scale probing
- Held-out predictions work → Design intervention experiments
- Clear mechanism emerging → Write theoretical paper

---

#### Fourteen: Hyle Architecture — Distributed Reasoning Graph

A system architecture for managing the experimental reasoning as a loopy graph of LLM agents.

**Core Concept:**
```
┌─────────────────────────────────────────────────────────────────┐
│                     HYLE REASONING CANVAS                        │
│                                                                  │
│   ┌─────────┐     ┌─────────┐     ┌─────────┐                   │
│   │ Agent A │────▶│ Agent B │────▶│ Agent C │──┐                │
│   │ (H1)    │     │ (H2)    │     │ (H3)    │  │                │
│   └────▲────┘     └─────────┘     └─────────┘  │                │
│        │                                        │                │
│        └────────────────────────────────────────┘ (loop)        │
│                                                                  │
│   Each agent: LLM + paragraph + state slice                     │
│   Edges: typed data flow (hashmap of measurements)              │
│   Managers: enforce formal properties between stages            │
└─────────────────────────────────────────────────────────────────┘
```

**The Loopy Graph:**

| Node | Agent Role | Paragraph Responsibility | State Slice |
|------|------------|-------------------------|-------------|
| `survey` | Literature scanner | "Related work shows..." | {papers: [], findings: []} |
| `fertility` | Tokenization analyst | "Fertility varies from..." | {lang→fertility} |
| `wals` | Typology mapper | "Morphological complexity..." | {lang→features} |
| `correlate` | Statistical engine | "Correlation analysis reveals..." | {r, R², p-values} |
| `predict` | Threshold modeler | "We predict threshold..." | {b*(L), coefficients} |
| `probe` | Layer analyst | "Layer-wise sensitivity..." | {layer→sensitivity} |
| `synthesize` | Paper assembler | "In conclusion..." | {full context} |

**Edges (typed data flows):**

```clojure
;; Edge types
{:measurement  {:value float :ci [lo hi] :source keyword :timestamp inst}
 :correlation  {:r float :p float :n int :variables [kw]}
 :prediction   {:target kw :value float :ci [lo hi] :features map}
 :history      {:variable kw :series [{:t inst :v float}]}
 :causal       {:from kw :to kw :strength float :mechanism string}}
```

**The Nested Hashmap (Experimental State):**

```clojure
(def experimental-state
  {:measurements
   {:fertility
    {"eng" {:value 1.2 :ci [1.1 1.4] :source :bloom :t #inst "2026-01-02"}
     "heb" {:value 2.1 :ci [1.8 2.4] :source :aya :t #inst "2026-01-02"}
     "ara" {:value 3.0 :ci [2.5 3.5] :source :bloom :t #inst "2026-01-02"}}
    :degradation
    {"eng" {:w4 {:value -0.02 :ci [-0.03 -0.01] :source :marchisio}}
     "ara" {:w4 {:value -0.12 :ci [-0.15 -0.08] :source :marchisio}}}}

   :correlations
   {:fertility-degradation {:r 0.72 :p 0.001 :n 23}
    :complexity-degradation {:r 0.45 :p 0.02 :n 23}
    :fertility-degradation|complexity {:r 0.58 :p 0.005 :n 23}}  ;; partial

   :models
   {:threshold-function
    {:form "b*(L) = b_base + α·fertility + β·complexity + γ·script"
     :params {:b_base 3.2 :alpha 0.4 :beta 0.08 :gamma 0.4}
     :R² 0.72
     :validated? false}}

   :causal-graph
   {:nodes [:fertility :complexity :script :training-data :degradation]
    :edges [{:from :fertility :to :degradation :type :direct}
            {:from :complexity :to :fertility :type :confound}
            {:from :training-data :to :degradation :type :confound}]}

   :history
   {:correlation-fertility-degradation
    [{:t #inst "2026-01-02T10:00" :r 0.65 :source :marchisio-only}
     {:t #inst "2026-01-02T14:00" :r 0.72 :source :marchisio+aya}]}})
```

**Agent Loop Structure:**

```clojure
(defn agent-loop [agent-id state-atom context-graph]
  (loop [iteration 0]
    (let [;; 1. Read current state slice
          my-state (get-in @state-atom [:slices agent-id])

          ;; 2. Read upstream outputs (edges into this node)
          upstream (get-upstream-outputs context-graph agent-id)

          ;; 3. Compose prompt from state + upstream + role
          prompt (compose-prompt agent-id my-state upstream)

          ;; 4. Call LLM (slow-burning, may take minutes)
          response (llm-call prompt {:model :sonnet :max-tokens 2000})

          ;; 5. Parse structured output
          {:keys [paragraph measurements updates]} (parse-response response)

          ;; 6. Validate against formal properties
          valid? (validate-output agent-id response)

          ;; 7. Update state atom (atomic swap)
          _ (when valid?
              (swap! state-atom
                     (fn [s]
                       (-> s
                           (assoc-in [:paragraphs agent-id] paragraph)
                           (update :measurements merge-measurements measurements)
                           (update :history conj-history agent-id iteration)))))

          ;; 8. Signal downstream agents
          _ (when valid?
              (notify-downstream context-graph agent-id))

          ;; 9. Check termination
          done? (or (> iteration 100)
                    (converged? @state-atom agent-id))]

      (if done?
        @state-atom
        (do (Thread/sleep 30000)  ;; slow-burn: 30s between iterations
            (recur (inc iteration)))))))
```

**Manager Layer (Formal Property Enforcement):**

```clojure
(def manager-rules
  {:survey→fertility
   {:rule "fertility-agent must cite at least 3 measurements from survey"
    :check (fn [survey-out fertility-out]
             (>= (count (intersection
                         (:sources survey-out)
                         (:sources fertility-out)))
                 3))}

   :correlate→predict
   {:rule "predictions must use correlations with p < 0.05"
    :check (fn [correlate-out predict-out]
             (every? #(< (:p %) 0.05)
                     (get-used-correlations predict-out correlate-out)))}

   :all-paragraphs
   {:rule "no paragraph may contradict another's measurements"
    :check (fn [paragraphs]
             (consistent-measurements? paragraphs))}})

(defn manager-loop [state-atom agents]
  (loop []
    (let [outputs (get-all-outputs @state-atom agents)]
      ;; Check all rules
      (doseq [[rule-id {:keys [check]}] manager-rules]
        (when-not (check outputs)
          ;; Signal violation to relevant agents
          (signal-violation! state-atom rule-id)))

      ;; Check global convergence
      (when-not (globally-converged? @state-atom)
        (Thread/sleep 60000)
        (recur)))))
```

**Canvas Composition (Article Assembly):**

```clojure
(defn compose-article [state]
  (let [paragraphs (:paragraphs state)
        order [:abstract :introduction :survey :methodology
               :fertility :wals :correlate :predict :probe
               :discussion :conclusion]]
    (->> order
         (map #(get paragraphs %))
         (filter some?)
         (interpose "\n\n")
         (apply str))))
```

**The Full System:**

```
┌───────────────────────────────────────────────────────────────────┐
│                        MANAGER LAYER                              │
│   ┌─────────────┐  ┌─────────────┐  ┌─────────────┐              │
│   │ Consistency │  │ Citation    │  │ Numerical   │              │
│   │ Manager     │  │ Manager     │  │ Manager     │              │
│   └──────┬──────┘  └──────┬──────┘  └──────┬──────┘              │
│          │                │                │                      │
│          ▼                ▼                ▼                      │
├───────────────────────────────────────────────────────────────────┤
│                        AGENT LAYER                                │
│                                                                   │
│   survey ──▶ fertility ──▶ correlate ──▶ predict                 │
│      │           │             │            │                     │
│      └───────────┴─────────────┴────────────┘ (feedback loops)   │
│                        │                                          │
│                        ▼                                          │
│                    synthesize                                     │
├───────────────────────────────────────────────────────────────────┤
│                        STATE LAYER                                │
│   ┌─────────────────────────────────────────────────────────────┐│
│   │ {:measurements {...} :correlations {...} :models {...}      ││
│   │  :causal-graph {...} :history {...} :paragraphs {...}}      ││
│   └─────────────────────────────────────────────────────────────┘│
├───────────────────────────────────────────────────────────────────┤
│                        OUTPUT LAYER                               │
│                                                                   │
│   ┌─────────────────────────────────────────────────────────────┐│
│   │                    ARTICLE.md                                ││
│   │  Abstract: [synthesize paragraph]                           ││
│   │  Introduction: [survey paragraph]                           ││
│   │  Results: [correlate + predict + probe paragraphs]          ││
│   │  Discussion: [synthesize paragraph]                         ││
│   └─────────────────────────────────────────────────────────────┘│
└───────────────────────────────────────────────────────────────────┘
```

**Properties of the System:**

| Property | How enforced |
|----------|--------------|
| **Consistency** | Manager checks measurements don't contradict |
| **Monotonicity** | New evidence can refine but not delete |
| **Traceability** | Every claim linked to measurement source |
| **Convergence** | Agents must stabilize within N iterations |
| **Formality** | Structured output schema per agent type |

**Implementation Path:**

| Phase | What | Tech |
|-------|------|------|
| 1 | Single-agent loop with state atom | Clojure + Claude API |
| 2 | Multi-agent with simple edges | core.async channels |
| 3 | Manager layer | spec/validation |
| 4 | Canvas rendering | Markdown generation |
| 5 | Persistence | Datomic or Datascript |
| 6 | Visualization | Re-frame + D3 |

---

### Part I: Empirical Foundation (What Already Exists)

Before designing experiments, we survey existing quantized models, benchmarks, and findings.

#### A. Existing Quantized Multilingual Models

| Model | Size | Languages | Quantized Versions | Source |
|-------|------|-----------|-------------------|--------|
| **BLOOM** | 176B | 46 spoken + 13 programming | GPTQ 4-bit (TheBloke) | [HuggingFace](https://huggingface.co/TheBloke/bloomz-176B-GPTQ) |
| **BLOOMChat** | 176B | Multilingual chat | GPTQ 4-bit | [HuggingFace](https://huggingface.co/TheBloke/BLOOMChat-176B-v1-GPTQ) |
| **mGPT** | 1.3B, 13B | 61 languages | 8-bit quantized | [HuggingFace](https://huggingface.co/monsoon-nlp/mGPT-13B-quantized) |
| **Aya-23** | 8B, 35B | 23 languages | AWQ, GGUF | [HuggingFace](https://huggingface.co/alijawad07/aya-23-8B-AWQ-GEMM) |
| **Qwen2.5** | 1.5B-72B | 29 languages | AWQ official | [HuggingFace](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct-AWQ) |
| **Command R/R+** | 35B, 103B | Multilingual | W4-g, W8, W8A8 | Cohere (studied in Marchisio et al.) |

**Key insight:** Quantized versions exist for most major multilingual models. We can analyze existing benchmark data before running new experiments.

---

#### B. Existing Benchmark Results: Marchisio et al. (EMNLP 2024)

The most comprehensive study to date. [Paper](https://arxiv.org/abs/2407.03211)

**Models tested:** Command R (35B), Command R+ (103B), Aya 23 (8B, 35B)

**Languages tested:** 10 primary (Arabic, French, German, English, Spanish, Italian, Portuguese, Korean, Japanese, Chinese) + 13 extended

**Quantization methods:**
- W8: 8-bit weight-only, per-column
- W8A8: 8-bit weight and activation
- W4-g: 4-bit weight-only with GPTQ, group-wise
- W4: 4-bit weight-only, per-column

**Key findings with specific numbers:**

| Language Group | Model | W8 | W8A8 | W4-g | W4 |
|----------------|-------|-----|------|------|-----|
| Latin/IE (de, es, fr, it, pt) | 103B | -0.1% | -0.7% | -0.4% | -0.7% |
| Non-Latin (ar, ja, ko, zh) | 103B | -0.1% | -0.9% | -1.5% | -1.9% |
| Latin/IE | 35B | — | — | -12.2% | — |
| Non-Latin | 35B | — | — | -14.4% | — |

**Human evaluation (critical finding):**
- Automatic metrics: Japanese -1.7% at W4-g
- Human evaluation: Japanese **-16.0%** at W4-g
- French: **-16.6%** at W4-g on challenging prompts

**Implication:** Automatic metrics severely underestimate degradation. Need human evaluation or more sensitive metrics.

---

#### C. Tokenization Fertility Data

Fertility = tokens per word. Higher = worse for that language.

| Language | Typical Fertility | Model | Source |
|----------|-------------------|-------|--------|
| English | 1.2-1.5 | Most models | Baseline |
| Japanese | 2.5+ | GPT-4 | Medium article |
| Arabic | 3.0+ | GPT-4 | "3× more tokens than English" |
| Ukrainian | 6.0+ | GPT-2, Phi-2 | Frontiers AI 2025 |
| Telugu, Odia | 11.7 | LLaMA-3 | ArXiv 2411.12240 |
| Chinese | High but varies | — | Logographic, different dynamics |

**Correlation with performance:**
- "Fertility explains 20-50% of variance in accuracy" (AfriMMLU study)
- "Doubling fertility leads to 4× increases in training cost"
- English-centric tokenizers cost up to 68% more for other languages

**Key insight:** Fertility is a strong predictor we can compute without running experiments.

---

#### D. Morphological Complexity Data (WALS)

[WALS Online](https://wals.info/) provides typological features for 2,600+ languages.

**Relevant features:**
| Feature | Code | Description |
|---------|------|-------------|
| Fusion of inflection | 20A | Isolating/concatenative/fusional |
| Prefixing vs suffixing | 26A | Suffixing/prefixing/both |
| Case marking | 49A | Number of grammatical cases |
| Word order | 81A | SOV/SVO/VSO/etc. |

**Lupyan-Dale Morphological Index:**
- Scores languages from -18 (no morphology) to 0 (maximal morphology)
- Based on 18 WALS features
- Available for 2,200+ languages

**Language complexity examples:**
| Language | Type | Complexity | Notes |
|----------|------|------------|-------|
| Mandarin | Isolating | Low | Few affixes, analytic |
| Vietnamese | Isolating | Low | Tonal, no morphology |
| English | Weakly fusional | Low-medium | Limited inflection |
| German | Fusional | Medium | Case, gender, number |
| Russian | Fusional | High | Rich case system |
| Arabic | Fusional + templatic | High | Root-pattern morphology |
| Hebrew | Fusional + templatic | High | Similar to Arabic |
| Turkish | Agglutinative | Very high | Many suffixes, regular |
| Finnish | Agglutinative | Very high | 15 cases, complex |
| Hungarian | Agglutinative | Very high | 18 cases |
| Inuktitut | Polysynthetic | Extreme | Sentence = one word |

---

#### E. Quantization Theory: Why Degradation Happens

**Core equation:** Quantization error Δx is irreversible information loss.

**Layer sensitivity predictors:**
1. **Hessian trace:** Higher curvature = more sensitive
2. **Output entropy:** Higher information = needs more bits
3. **Position:** Later layers more sensitive than early

**Key theoretical result (Soudry et al.):**
- Straight-Through Estimator enables gradient flow through quantization
- Provable bounds exist for single-layer quantization error
- FP4 feasible with proper techniques (2025 spotlights)

**Information-theoretic bound:**
```
optimal_bits(layer_i) ∝ H(output_layer_i)
```

Layers with higher entropy require proportionally more bits.

---

#### F. What Existing Research Tells Us

**Established facts:**
1. Non-Latin scripts degrade more (Marchisio et al.)
2. Fertility predicts accuracy (multiple studies)
3. Human eval shows 10× worse degradation than automatic metrics
4. Smaller models (<10B) degrade more than larger models
5. Challenging tasks (math reasoning) degrade fastest
6. GGUF most consistent across bit-widths

**Open questions (not yet tested):**
1. Does morphological complexity predict degradation *independently* of fertility?
2. Which layers are sensitive for which languages?
3. Does script type matter beyond fertility effects?
4. Can linguistic features predict precision requirements a priori?
5. Does morpheme-aware tokenization help?

---

### Part II: Refined Hypotheses

Based on empirical foundation above, we refine our hypotheses.

#### Hypothesis 1: Morphological Complexity Predicts Quantization Sensitivity

**Claim:** Languages with higher morphological complexity (measured by WALS features or Lupyan-Dale index) show greater perplexity degradation under FP4/FP8 quantization.

**Rationale:** Morphologically rich languages encode more grammatical information per token. When tokens are "compressed" via quantization, this information is more likely to be lost.

**Prediction:**
- Agglutinative languages (Turkish, Finnish, Hungarian) degrade more than isolating languages (Mandarin, Vietnamese)
- Fusional languages (Russian, Arabic, Hebrew) fall in between
- Correlation: r > 0.5 between WALS morphological complexity index and perplexity increase at W4

**Experiment 1.1: Cross-Linguistic Perplexity Analysis**

| Step | Action |
|------|--------|
| 1 | Select 20 languages spanning morphological types: 5 isolating, 5 agglutinative, 5 fusional, 5 polysynthetic |
| 2 | Use BLOOM-7B or Aya-23 (both have broad language coverage) |
| 3 | Quantize with AWQ at W8, W4, W3 precision |
| 4 | Measure perplexity on held-out Wikipedia text for each language |
| 5 | Calculate degradation: Δperplexity = (PPL_quantized - PPL_fp16) / PPL_fp16 |
| 6 | Correlate with Lupyan-Dale morphological complexity scores |

**Tools needed:**
- `transformers` + `auto-awq` for quantization
- WALS database for morphological features
- Flores-200 or CC-100 for evaluation data

**Expected output:** Scatter plot of complexity vs. degradation with regression line

---

### Hypothesis 2: Tokenization Fertility Mediates Quantization Harm

**Claim:** Languages with higher tokenization fertility (more tokens per word) suffer greater quantization degradation, independent of morphological complexity.

**Rationale:** High fertility means more "decision points" per word → more opportunities for quantization error to accumulate.

**Prediction:**
- Fertility (tokens/word) predicts degradation better than training data size
- After controlling for fertility, morphological complexity still has residual effect

**Experiment 2.1: Fertility-Controlled Analysis**

| Step | Action |
|------|--------|
| 1 | Measure fertility for 30 languages in BLOOM/Aya vocabulary |
| 2 | Quantize and measure perplexity as in Experiment 1.1 |
| 3 | Fit regression: Δperplexity ~ fertility + complexity + training_data_size |
| 4 | Report standardized coefficients for each predictor |

**Experiment 2.2: Matched-Fertility Comparison**

| Step | Action |
|------|--------|
| 1 | Find language pairs with similar fertility but different complexity: e.g., German (fusional) vs. Dutch (less fusional) |
| 2 | Compare quantization degradation within pairs |
| 3 | If complexity matters beyond fertility, pairs should differ |

---

### Hypothesis 3: Attention Layers Are Differentially Sensitive Across Languages

**Claim:** Attention layers are more sensitive to quantization for languages with flexible word order, while FFN layers are more sensitive for morphologically complex languages.

**Rationale:**
- Attention encodes positional/structural relationships → word order flexibility requires higher precision
- FFN encodes lexical/semantic information → morphological complexity requires higher precision

**Prediction:**
- For SOV languages (Japanese, Turkish, Korean): attention layers need higher precision
- For morphologically rich languages: FFN layers need higher precision

**Experiment 3.1: Layer-wise Sensitivity by Language**

| Step | Action |
|------|--------|
| 1 | For each language, apply mixed-precision quantization |
| 2 | Try: Attention at FP8, FFN at FP4 (and vice versa) |
| 3 | Measure perplexity and downstream task accuracy |
| 4 | Build language × layer sensitivity matrix |

**Experiment 3.2: Attention Pattern Analysis**

| Step | Action |
|------|--------|
| 1 | Extract attention weights before and after quantization |
| 2 | Compute Jensen-Shannon divergence between attention distributions |
| 3 | Correlate divergence with word order flexibility (WALS feature 81A) |

---

### Hypothesis 4: Script Type Interacts with Quantization

**Claim:** Languages using logographic scripts (Chinese, Japanese kanji) are more robust to quantization than alphabetic/syllabic scripts.

**Rationale:** Logographic tokens are semantically denser (one token ≈ one morpheme), so fewer tokens need to "cooperate" for meaning. Quantization errors are more isolated.

**Prediction:**
- Mandarin Chinese degrades less than Korean (syllabic) which degrades less than Hindi (alphabetic with complex conjuncts)

**Experiment 4.1: Script-Controlled Comparison**

| Step | Action |
|------|--------|
| 1 | Select language triplets: same family, different scripts (e.g., Japanese written in kanji vs. hiragana) |
| 2 | Or: different families, same script type |
| 3 | Quantize and compare |

---

### Hypothesis 5: Quantization-Aware Morphological Tokenization Helps

**Claim:** A tokenizer trained to respect morpheme boundaries will produce representations more robust to quantization than BPE.

**Rationale:** BPE splits arbitrarily based on frequency; morpheme-aware tokenization creates linguistically meaningful units that may have more stable representations.

**Prediction:**
- Morfessor-based tokenization + quantization outperforms BPE + quantization for morphologically rich languages
- No difference for isolating languages (no morphology to respect)

**Experiment 5.1: Tokenizer Comparison**

| Step | Action |
|------|--------|
| 1 | Train small LM (350M) with BPE vs. Morfessor tokenizer on Finnish/Turkish |
| 2 | Quantize both to FP4 |
| 3 | Compare perplexity degradation |

---

### Experimental Infrastructure

**Models to use:**
| Model | Size | Languages | Why |
|-------|------|-----------|-----|
| BLOOM-7B | 7B | 46 languages | Broad coverage, well-documented |
| Aya-23 | 8B | 23 languages | Recent, instruction-tuned |
| mT5-XL | 3.7B | 101 languages | Encoder-decoder, different architecture |
| Llama-3-8B | 8B | English-centric | Baseline comparison |

**Quantization methods:**
| Method | Precision | Use case |
|--------|-----------|----------|
| AWQ | W4, W8 | Primary method (fast, good quality) |
| GPTQ | W4, W3, W2 | Extreme compression |
| bitsandbytes | 4-bit, 8-bit | QLoRA fine-tuning |

**Evaluation metrics:**
| Metric | What it measures | How to compute |
|--------|------------------|----------------|
| Perplexity | Language modeling quality | `model.eval()` on held-out text |
| Fertility | Tokenization efficiency | tokens / words in reference corpus |
| BLEU (translation) | Task-specific quality | SacreBLEU on Flores-200 |
| ΔAccuracy | Downstream degradation | XQuAD, XNLI, XCOPA benchmarks |

**Linguistic features (from WALS):**
| Feature | Code | What it captures |
|---------|------|------------------|
| Morphological complexity | Lupyan-Dale index | Overall morphological richness |
| Word order | 81A-83A | SOV, SVO, etc. |
| Case marking | 49A | Presence/complexity of case system |
| Number of cases | 49A | 2, 3-4, 5-6, 7-8, 9+ |
| Fusion of case/number | 21A | Separate vs. fused affixes |

---

### Compute Requirements

| Experiment | GPU hours (A100) | Notes |
|------------|------------------|-------|
| 1.1 (20 languages × 3 precisions) | ~20h | Quantization is fast, eval is cheap |
| 2.1 (30 languages) | ~30h | Same as 1.1 |
| 3.1 (layer-wise, 10 languages) | ~50h | Many configurations to test |
| 5.1 (train 350M model) | ~100h | From scratch training |

**Total for pilot study:** ~200 GPU hours ≈ $400-600 at cloud rates

---

### Part III: Zero-Cost Analysis (Before Running Experiments)

Before spending compute, we can extract substantial insight from existing data.

#### Analysis 0.1: Re-Analyze Marchisio et al. Data

**Data available:** Per-language degradation for 23 languages across 4 quantization methods.

**What we can compute:**
1. Correlate their results with WALS morphological features
2. Correlate with tokenization fertility (compute from their models)
3. Partial correlation: morphology controlling for fertility
4. Script type as categorical predictor

**Time:** ~1 day of data wrangling, zero compute

---

#### Analysis 0.2: Compute Fertility for All Languages

**Method:** Run tokenizers of BLOOM, Aya, Qwen on parallel corpora (Flores-200, WikiMatrix)

**What we get:** Fertility scores for 100+ languages

**Can then correlate with:**
- Existing benchmark scores (Open LLM Leaderboard)
- Intel Low-Bit Leaderboard results
- WALS morphological complexity

**Time:** ~2 days, minimal compute (just tokenization)

---

#### Analysis 0.3: Download and Analyze Intel Leaderboard

[Intel Low-Bit Quantized Leaderboard](https://huggingface.co/spaces/Intel/low_bit_open_llm_leaderboard)

**Available data:**
- Scores for 10 benchmarks across many quantized models
- Filter by quantization method (AutoRound, GPTQ, AWQ, bitsandbytes, GGUF)
- Filter by bit-width (int4, int8, fp4, etc.)

**What we can extract:**
- Performance variance across models at different bit-widths
- Which quantization methods most consistent
- Baseline for expected degradation

**Time:** ~1 day

---

#### Analysis 0.4: Survey HuggingFace Model Cards

**Method:** Scrape/download model cards for quantized multilingual models

**What to look for:**
- Any reported benchmark scores
- Training data composition (language proportions)
- Evaluation on non-English tasks

**Models to check:**
- TheBloke's BLOOM variants
- Qwen2.5 AWQ variants
- Aya quantized variants
- mGPT quantized

**Time:** ~2 days manual review

---

#### Analysis 0.5: Build Language-Feature Database

**Compile:**
| Language | ISO | WALS Complexity | Fertility (BLOOM) | Fertility (Aya) | Script | Family |
|----------|-----|-----------------|-------------------|-----------------|--------|--------|
| English | eng | -12 | 1.2 | 1.3 | Latin | Germanic |
| Hebrew | heb | -4 | ? | 2.1 | Hebrew | Semitic |
| Arabic | ara | -3 | ? | 3.0 | Arabic | Semitic |
| Finnish | fin | -2 | ? | ? | Latin | Uralic |
| Turkish | tur | -1 | ? | ? | Latin | Turkic |
| ... | ... | ... | ... | ... | ... | ... |

**Purpose:** Foundation for all subsequent analysis

**Time:** ~3 days to compile properly

---

#### Zero-Cost Analysis Summary

| Analysis | Data Source | Time | Insight |
|----------|-------------|------|---------|
| 0.1 Re-analyze Marchisio | Their paper + WALS | 1 day | Test H1 immediately |
| 0.2 Compute fertility | Tokenizers + corpora | 2 days | Foundation for H2 |
| 0.3 Intel Leaderboard | HuggingFace | 1 day | Baseline expectations |
| 0.4 Model card survey | HuggingFace | 2 days | Find gaps in literature |
| 0.5 Language database | WALS + computed | 3 days | Foundation for everything |

**Total zero-cost phase:** ~9 days of work, produces:
- Preliminary test of H1 and H2
- Clear picture of what's missing
- Refined hypotheses for compute-intensive experiments

---

### Priority Order (After Zero-Cost Analysis)

| Priority | Experiment | Risk | Payoff | Time |
|----------|------------|------|--------|------|
| 1 | 1.1 (morphological complexity) | Low | High | 1 week |
| 2 | 2.1 (fertility analysis) | Low | Medium | 1 week |
| 3 | 3.1 (layer-wise sensitivity) | Medium | High | 2 weeks |
| 4 | 4.1 (script analysis) | Low | Medium | 1 week |
| 5 | 5.1 (tokenizer comparison) | High | High | 4 weeks |

**Recommended starting point:** Experiments 1.1 and 2.1 can be run immediately with existing models and no training. Results will guide whether to pursue the more complex experiments.

---

### Potential First Paper Structure

**Title:** "Does Hebrew Need More Bits? Linguistic Typology Predicts LLM Quantization Sensitivity"

**Abstract (draft):**
> Low-bit quantization (FP4/FP8) enables efficient LLM deployment but affects languages unequally. We show that morphological complexity, tokenization fertility, and script type predict quantization degradation across 30 languages. Languages with rich morphology (Hebrew, Finnish, Turkish) require 1.5-2× higher precision in attention layers to match English performance. We propose a linguistically-informed precision allocation strategy that recovers 90% of FP16 quality at FP4 compute cost for multilingual models.

**Sections:**
1. Introduction: Multilingual LLMs + quantization = underexplored
2. Background: Quantization methods, linguistic typology
3. Experiments 1-4: Correlational analyses
4. Experiment 5: Intervention (tokenizer)
5. Practical recommendations: Precision allocation heuristics
6. Conclusion: Linguistic knowledge improves efficiency

**Target venues:** EMNLP 2027, ICLR 2027, or ACL 2027

---

## Top 12 Project Ideas (Developed)

*Well-specified problems with clear objectives, established methods, and active advisors. Start here.*

### 1. Vocabulary Optimization for Efficient LLMs

**Advisor:** Roy Schwartz (HUJI) ✅ actively recruiting

**The problem:** LLM inference cost scales with vocabulary size, but vocabulary design is understudied. Schwartz's group has shown optimal vocabularies can reduce compute while maintaining performance.

**Why tractable:**
- Clear metric: tokens/second, energy/query
- Established benchmarks (GLUE, SuperGLUE)
- Active advisor with published methodology

**Research questions:**
1. What is the optimal vocabulary size for different tasks and languages?
2. How do subword tokenizers affect downstream efficiency?
3. Can vocabulary be dynamically adjusted during inference?

**Your fit:** Linguistics background gives intuition for morphological tokenization; physics background enables optimization analysis.

**Next step:** Read [Schwartz Lab publications](https://schwartz-lab-huji.github.io/publications/) and email Roy with specific questions about vocabulary research.

---

### 2. Green AI: Carbon-Aware Training

**Advisor:** Roy Schwartz (HUJI) ✅ actively recruiting

**The problem:** Training large models has significant environmental cost. Schwartz's highly-cited "Green AI" paper (2020) established the research agenda for minimizing compute while maintaining capability.

**Why tractable:**
- Measurable: energy per accuracy point
- Established benchmarks and methodology
- Growing field with industry interest

**Research questions:**
1. How do we measure the true energy cost of NLP systems?
2. Can carbon-aware training schedules reduce environmental impact?
3. What architectural choices minimize compute-per-accuracy?

**Key paper:** [Green AI (Schwartz et al., 2020)](https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext) — 3000+ citations

**Next step:** Read the Green AI paper and recent Schwartz lab work on efficiency.

---

### 3. Optimization Landscape of Deep Learning

**Advisor:** Daniel Soudry (Technion) ✅ actively recruiting

**The problem:** Why do neural networks trained with gradient descent generalize well despite having many more parameters than training examples? The geometry of the loss landscape holds answers.

**Why tractable:**
- Physics-native problem (energy surfaces, critical points)
- Strong theoretical tradition
- Soudry has multiple published results

**Research questions:**
1. What geometric properties predict trainability?
2. How does overparameterization affect landscape structure?
3. Can we design architectures with favorable landscapes?

**Your fit:** Physics background maps directly to loss landscape analysis (think: statistical mechanics of learning).

**Key papers:** Check [Soudry's Google Scholar](https://scholar.google.com/citations?user=soudry) for recent loss landscape work.

---

### 4. Developmental Interpretability

**Advisor:** Daniel Soudry (Technion) ✅ actively recruiting

**The problem:** Most interpretability research studies trained models. This emerging subfield studies how neural network mechanisms **form during training** — like developmental biology for AI.

**Why tractable:**
- Well-defined: track circuit formation over training
- Active advisor with relevant expertise
- Connects to Soudry's work on training dynamics

**Research questions:**
1. What phase transitions occur during training?
2. Can we predict final model behavior from early training dynamics?
3. How do circuits emerge and stabilize?

**Key paper:** [Developmental Interpretability (arXiv 2508.15841)](https://arxiv.org/abs/2508.15841)

---

### 5. Hebrew NLP: Beyond Morpho-Syntactic Tasks

**Advisor:** Reut Tsarfaty (BIU) — ERC-funded NLPRO project

**The problem:** Current Hebrew NLP benchmarks focus on morphology and syntax. Semantic understanding (reading comprehension, inference, QA) is underdeveloped. BIU-NLP is building Hebrew MRC datasets.

**Why tractable:**
- Clear gap in existing benchmarks
- ERC-funded lab with resources
- Your linguistics background is directly relevant

**Research questions:**
1. How can morphological analysis improve semantic task performance in Hebrew?
2. Can cross-lingual transfer from Arabic (another Semitic language) help?
3. What evaluation metrics are appropriate for morphologically-rich languages?

**Key resource:** [BIU-NLP Lab](https://biu-nlp.github.io/)

---

### 6. Morphological Analysis for Low-Resource Languages

**Advisor:** Reut Tsarfaty (BIU)

**The problem:** Low-resource languages with rich morphology (Dogri, Luganda, Hebrew dialects) benefit from hybrid approaches combining rules with neural methods. Recent work shows 35-38% BLEU improvement from morphological features.

**Why tractable:**
- Clear methodology exists
- Measurable improvements (BLEU)
- Builds on linguistics expertise

**Research questions:**
1. Can we design tokenization that respects morphological boundaries?
2. How do morphological features improve translation quality?
3. Can morphological structure guide attention mechanisms?

**Key paper:** [Hybrid morphological analysis (Springer 2025)](https://link.springer.com/article/10.1007/s42979-025-04429-9)

---

### 7. Information Bottleneck in Deep Learning

**Advisor:** Nati Linial (HUJI)

**The problem:** The information bottleneck principle suggests networks compress inputs while preserving task-relevant information. This connects information theory to deep learning generalization.

**Why tractable:**
- Physics-native (information theory)
- Clear theoretical questions
- Strong tradition at HUJI

**Research questions:**
1. Does training actually optimize the information bottleneck?
2. How does architecture affect information flow?
3. Can we design architectures that explicitly optimize information objectives?

**Your fit:** Physics background provides information theory intuition.

---

### 8. Random Matrices and Deep Learning

**Advisors:** Ofer Zeitouni, Boaz Klartag (Weizmann) ✅ recruiting

**The problem:** Random matrix theory provides tools for understanding high-dimensional data and neural network training dynamics. Weizmann has world-class probability faculty.

**Why tractable:**
- Physics background directly relevant
- World-class advisors
- Clear mathematical framework

**Research questions:**
1. What are the universality limits of random matrix eigenvalue statistics?
2. How do random matrices connect to neural network training dynamics?
3. Can random matrix theory explain deep learning phenomena?

**Your fit:** Physics + math background is ideal for this direction.

---

### 9. Desalination Process Optimization

**Institution:** BGU Zuckerberg Institute for Water Research

**The problem:** Israel leads the world in desalination (~70% of domestic water). ML can reduce energy consumption and predict membrane fouling.

**Why tractable:**
- Clear objective: energy per cubic meter
- Real-world data available
- Israel is global leader with industry connections

**Research questions:**
1. Can we predict membrane fouling before it occurs?
2. What control strategies minimize energy consumption?
3. How do we handle varying water quality inputs?

**Your fit:** Physics background enables process modeling; optimization is well-defined.

---

### 10. Climate Modeling with Physics-Informed Neural Networks

**Institution:** Weizmann Earth and Planetary Sciences

**The problem:** Climate models are computationally expensive. Physics-informed neural networks (PINNs) can incorporate conservation laws while learning from data.

**Why tractable:**
- Physics-informed approach is your strength
- Growing field with clear methodology
- Climate relevance ensures funding

**Research questions:**
1. How should conservation laws be encoded in climate models?
2. Can neural networks learn subgrid-scale processes?
3. What uncertainty quantification is achievable?

---

### 11. Code-Switching in Hebrew-English Text

**Advisor:** Reut Tsarfaty (BIU)

**The problem:** Israeli texts freely mix Hebrew and English ("Hebrish"). Code-switching NLP must handle mixed scripts and rapid language alternation.

**Why tractable:**
- Underexplored but practical
- Clear evaluation on social media data
- Builds on morphological expertise

**Research questions:**
1. How should tokenizers handle code-switched text?
2. Can language ID improve downstream NLP tasks?
3. What linguistic patterns govern code-switching in Israeli social media?

---

### 12. Precision Irrigation from Sensor Data

**Institution:** BGU Zuckerberg Institute for Water Research

**The problem:** Water-efficient agriculture is critical in arid regions. ML can optimize irrigation scheduling from soil moisture, weather, and crop models.

**Why tractable:**
- Clear objective: water efficiency
- Israel is world leader in drip irrigation
- Measurable real-world impact

**Research questions:**
1. How should sensor data and weather forecasts be combined?
2. Can we learn crop-specific water needs?
3. What is the optimal sensor density for different farm sizes?

---

## Remaining Project Ideas (Scored)

*73 additional ideas, pruned from original 128. Grouped by theme with tractability score.*

### NLP & Language (18 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 1 | Mechanistic interpretability of LLMs | Medium | Goldberg, Reichart |
| 3 | String counterfactuals for robustness | High | Goldberg, Dagan |
| 4 | Zero-shot NER via type embeddings | High | Goldberg |
| 5 | Few-shot learning: diversity over quantity | Medium | Goldberg, Mansour |
| 47 | Jailbreak attack analysis | Medium | Goldberg, Schwartz |
| 48 | Adversarial robustness in Hebrew/Arabic | High | Tsarfaty, Goldberg |
| 49 | Uncertainty quantification in LLMs | High | Schwartz, Soudry |
| 56 | Palestinian Arabic NLP | Medium | Tsarfaty |
| 57 | Hebrew-Arabic cross-lingual transfer | High | Tsarfaty |
| 58 | Diacritization for Semitic languages | High | Tsarfaty |
| 59 | Neuro-symbolic reasoning | Medium | Dagan, Abend |
| 60 | Compositional generalization | Medium | Abend, Goldberg |
| 61 | Temporal reasoning in NLP | Medium | Dagan |
| 62 | Knowledge graph + LLM integration | Medium | Dagan, Shapira |
| 79 | Clinical NLP for Hebrew | Medium | Tsarfaty |
| 80 | Misinformation detection (multilingual) | Medium | Goldberg, Elovici |
| 81 | Educational NLP assessment | Medium | NLP faculty |
| 82 | Legal NLP for Israeli law | Medium | Dagan |

### ML Theory (12 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 7 | Sample complexity of online RL | Medium | Mannor, Mansour |
| 8 | Adversarially robust PAC learning | Medium | Mansour, Linial |
| 9 | Margin bounds for voting classifiers | Medium | Mansour, Globerson |
| 10 | Kolmogorov complexity and generalization | Medium | Linial, Goldreich |
| 11 | Sample complexity of diffusion models | High | Globerson, Soudry |
| 41 | Efficient training at scale | High | Soudry, Globerson |
| 43 | Continual learning theory | Medium | Mannor, ELSC |
| 70 | Unlearning in LLMs | High | Soudry, Goldberg |
| 74 | Manifold learning for high-D data | Medium | Klartag, Soudry |
| 75 | Information bottleneck | High | Linial, Soudry |
| 76 | MDL and neural networks | Medium | Linial, Mansour |
| 78 | Rate-distortion for generative models | Medium | Globerson, Soudry |

### Mathematics & TCS (8 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 12 | Additive combinatorics + communication | Medium | Ziegler |
| 14 | Metric geometry and algorithms | Medium | Naor |
| 15 | GNNs for combinatorial optimization | High | Kimmel, Ailon |
| 16 | Diffusion for combinatorial optimization | Medium | Mannor, Barequet |
| 36 | Graph algorithms + learning | Medium | Elkin, Linial |
| 37 | LLM reasoning on graphs | Medium | NLP + TCS faculty |
| 38 | Computational geometry + vision | Medium | Kimmel, Barequet |
| 66 | LLM-assisted theorem proving | Low | Math faculty |

### Neuroscience & Cognition (6 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 21 | LLMs as brain models | Medium | Globerson, ELSC |
| 22 | Brain encoding/decoding | Medium | ELSC, NLP faculty |
| 23 | Neuromorphic language models | Low | ELSC, Technion EE |
| 51 | Grounded language learning | Medium | Abend, Wolf |
| 52 | Video understanding | Medium | Irani, Wolf |
| 53 | Audio-visual speech (Semitic) | Medium | ELSC |

### Energy & Climate (8 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 83 | Solar cell materials discovery | Medium | Weizmann Chemistry |
| 86 | Smart grid optimization | High | Mannor |
| 87 | Agricultural yield prediction | Medium | Weizmann Earth Sci |
| 106 | Battery materials discovery | Medium | Technion Materials |
| 107 | Hydrogen fuel cell catalysts | Medium | Weizmann Chemistry |
| 108 | Building energy digital twins | Medium | Technion Civil |
| 109 | Wind farm optimization | Medium | Mannor |
| 110 | Perovskite solar stability | Medium | Weizmann Chemistry |

### Biotech & Chemistry (12 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 88 | Molecular property prediction (GNNs) | High | Kimmel |
| 89 | Protein structure + design | Medium | Weizmann Structural Bio |
| 90 | Retrosynthesis planning | Medium | Weizmann Chemistry |
| 91 | Drug-target interaction | Medium | Goldberg, Weizmann |
| 92 | Single-cell RNA analysis | Medium | Weizmann Bio Regulation |
| 93 | CRISPR guide RNA design | Medium | Harel, Weizmann |
| 94 | Microbiome-host modeling | Medium | Weizmann Immunology |
| 95 | Antibiotic resistance prediction | Medium | Weizmann, BGU |
| 119 | Vaccine target identification | Medium | Weizmann Immunology |
| 121 | Personalized nutrition | Medium | Weizmann (Segal) |
| 127 | Organ-on-chip analysis | Low | Technion Biomedical |
| 128 | Stem cell differentiation | Low | Weizmann MCB |

### Privacy & Security (5 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 31 | Cyber security via NLP | Medium | Elovici, Goldberg |
| 67 | Differentially private NLP | Medium | Mansour, Schwartz |
| 68 | Federated multilingual learning | Medium | Mannor, Mansour |
| 69 | Membership inference attacks | Medium | Elovici |
| 98 | Federated learning for medical imaging | Medium | Mansour |

### Sustainability & Society (4 ideas)

| # | Idea | Tractability | Advisor |
|---|------|--------------|---------|
| 102 | Assistive technology / accessibility | Medium | NLP faculty |
| 103 | Archaeological site detection | Medium | Irani, HUJI Archaeology |
| 104 | Endangered language documentation | Medium | Tsarfaty |
| 105 | Humanitarian earth observation | Medium | Technion EE |

---

## Priority Contact List

Based on recruitment status and research fit:

**Tier 1 — Explicitly Recruiting:**
1. Daniel Soudry (Technion) — daniel.soudry@gmail.com
2. Roy Schwartz (HUJI) — schwartz@cs.huji.ac.il
3. Weizmann Math/CS — Apply through Feinberg School

**Tier 2 — Active Labs:**
4. Reut Tsarfaty (BIU) — Hebrew NLP
5. Amir Globerson (TAU) — ML theory
6. Shie Mannor (Technion) — RL theory
7. Ido Dagan (BIU) — NLP semantics

**Tier 3 — Conditional:**
8. Yoav Goldberg (BIU) — High standards, read his note first
9. Yonatan Belinkov (Technion) — For 2026-27 start only

---

## Funding

| Source | Amount | Notes |
|--------|--------|-------|
| **Weizmann** | Free + stipend | All admitted students |
| **HUJI Mandel** | 75,000 NIS/year | 4 years max |
| **Excellence Fellowship** | 168,000 NIS/year | Competitive |
| **Fulbright Israel** | Full support | US citizens |
| **Azrieli Visiting** | 12,000 NIS/month | Short visits |

---

## Note on Peaceful Research

*All project ideas in this document focus on civilian, peaceful applications. Topics with potential dual-use concerns have been excluded. Research should serve humanitarian goals: health, sustainability, knowledge, and human flourishing.*

---

*For art residencies and senior tech positions, see [Levant Resources](/levant/)*
